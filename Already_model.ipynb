{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qRpEfhxgFach"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wJyFA6sZPqEh",
        "outputId": "58d34e94-d7e0-45dd-c440-e3fc04f96cb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datas = np.load('/content/drive/MyDrive/final_data/dataset.npy')\n",
        "labels = np.load('/content/drive/MyDrive/final_data/labels.npy')"
      ],
      "metadata": {
        "id": "UQ4QW-7aFk1k"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datas.shape, labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0iL4-8gPzP7",
        "outputId": "01b12aa1-9cc1-44a0-e34f-74ab836d1b44"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2954, 20, 100, 100), (2954,))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_datas = np.load('/content/drive/MyDrive/final_data/val_dataset.npy')\n",
        "val_labels = np.load('/content/drive/MyDrive/final_data/val_labels.npy')"
      ],
      "metadata": {
        "id": "5iT9131rPh8g"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_datas.shape, val_labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOAsEVEFP8xV",
        "outputId": "f2e310b1-3337-4d89-ac43-60fa077ac6a9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1981, 20, 100, 100), (1981,))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@keras.saving.register_keras_serializable()\n",
        "class EMA_Layer(tf.keras.layers.Layer):\n",
        "    def __init__(self, alpha=0.9, **kwargs):\n",
        "        super(EMA_Layer, self).__init__(**kwargs)\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def call(self, x):\n",
        "        split = tf.split(x, x.shape[-2], axis=-2)\n",
        "        ema_tensor = split[0]\n",
        "        for i in range(1, x.shape[-2]):\n",
        "            ema_tensor = tf.concat([ema_tensor, split[i] * self.alpha + ema_tensor[... , -1:, :] * (1 - self.alpha)], axis=-2)\n",
        "\n",
        "        return ema_tensor\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(EMA_Layer, self).get_config()\n",
        "        config.update({\n",
        "            'alpha': self.alpha\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "5pAeWsWazHA3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@keras.saving.register_keras_serializable()\n",
        "class AttentionFrame(tf.keras.layers.Layer):\n",
        "  def __init__(self, alpha, **kwargs) -> None:\n",
        "    super().__init__(**kwargs)\n",
        "    self.alpha = alpha\n",
        "    self.EMA1 = EMA_Layer(alpha = alpha)\n",
        "    self.EMA2 = EMA_Layer(alpha = alpha*1.5)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.w_attn = tf.keras.layers.EinsumDense('...b,bc->...c', output_shape=[input_shape[1]], activation='sigmoid', bias_axes='c')\n",
        "    self.idx_attn = tf.keras.Sequential(\n",
        "        [\n",
        "            tf.keras.layers.Dense(int(input_shape[1]), activation='tanh'),\n",
        "            tf.keras.layers.Dense(int(input_shape[1]), activation='relu'),\n",
        "            tf.keras.layers.Dense(input_shape[1], activation='linear'),\n",
        "            tf.keras.layers.Flatten(),\n",
        "            tf.keras.layers.LayerNormalization()\n",
        "        ]\n",
        "    )\n",
        "    super().build(input_shape)\n",
        "\n",
        "  def call(self, input):\n",
        "    output = self.EMA1(input)\n",
        "    output += self.EMA2(output)\n",
        "    output = self.idx_attn(output)\n",
        "    output = self.w_attn(output)\n",
        "    output = output[..., tf.newaxis] * input\n",
        "    return output\n",
        "\n",
        "  def get_config(self):\n",
        "        config = super(AttentionFrame, self).get_config()\n",
        "        config.update({\n",
        "            'alpha': self.alpha,\n",
        "            'EMA1': self.EMA1,\n",
        "            'EMA2': self.EMA2,\n",
        "            'w_attn': self.w_attn,\n",
        "            'idx_attn':self.idx_attn\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "8OQRt9BaGZ-a"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@keras.saving.register_keras_serializable()\n",
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, **kwargs) -> None:\n",
        "    super().__init__(**kwargs)\n",
        "    self.d_model = d_model\n",
        "    self.dff = dff\n",
        "\n",
        "    self.LN = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.cvrt = tf.keras.layers.EinsumDense('...b,bc->...c', output_shape=[self.d_model], activation='relu', bias_axes='c')\n",
        "    self.position = self.add_weight(name=\"position\", shape=([1, input_shape[1]]),\n",
        "                              initializer=tf.initializers.Constant(tf.range(1., input_shape[1] + 1)),\n",
        "                              trainable=False)\n",
        "    self.MLP_pos = tf.keras.Sequential(\n",
        "        [\n",
        "            tf.keras.layers.Dense(self.d_model, activation=\"tanh\"),\n",
        "            tf.keras.layers.Dense(self.dff, activation=\"relu\"),\n",
        "            tf.keras.layers.Dense(input_shape[1], activation=\"sigmoid\")\n",
        "        ]\n",
        "    )\n",
        "    super().build(input_shape)\n",
        "\n",
        "  def call(self, input):\n",
        "    position = self.MLP_pos(self.position)\n",
        "    output = self.cvrt(input)\n",
        "    output = tf.add(output, tf.squeeze(position)[..., tf.newaxis])\n",
        "    output = self.LN(output)\n",
        "    return output\n",
        "\n",
        "  def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            'd_model': self.d_model,\n",
        "            'dff': self.dff,\n",
        "            'LN': self.LN,\n",
        "            'cvrt':self.cvrt,\n",
        "            'position': self.position.numpy().tolist(),\n",
        "            'MLP_pos':self.MLP_pos\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "71X8kfb_o_gR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "@keras.saving.register_keras_serializable()\n",
        "class MultiHeadAttention_OVR(tf.keras.layers.MultiHeadAttention):\n",
        "  def __init__(self, num_heads: int, key_dim: int, decay: float, **kwargs):\n",
        "    super().__init__(num_heads, key_dim, **kwargs)\n",
        "    self.EMA = EMA_Layer(decay)\n",
        "\n",
        "\n",
        "  def _compute_attention(\n",
        "        self, query, key, value, attention_mask=None, training=None\n",
        "    ):\n",
        "        \"\"\"Applies Dot-product attention with query, key, value tensors.\n",
        "\n",
        "        This function defines the computation inside `call` with projected\n",
        "        multi-head Q, K, V inputs. Users can override this function for\n",
        "        customized attention implementation.\n",
        "\n",
        "        Args:\n",
        "            query: Projected query `Tensor` of shape `(B, T, N, key_dim)`.\n",
        "            key: Projected key `Tensor` of shape `(B, S, N, key_dim)`.\n",
        "            value: Projected value `Tensor` of shape `(B, S, N, value_dim)`.\n",
        "            attention_mask: a boolean mask of shape `(B, T, S)`, that prevents\n",
        "                attention to certain positions. It is generally not needed if\n",
        "                the `query` and `value` (and/or `key`) are masked.\n",
        "            training: Python boolean indicating whether the layer should behave\n",
        "                in training mode (adding dropout) or in inference mode (doing\n",
        "                nothing).\n",
        "\n",
        "        Returns:\n",
        "          attention_output: Multi-headed outputs of attention computation.\n",
        "          attention_scores: Multi-headed attention weights.\n",
        "        \"\"\"\n",
        "        # Note: Applying scalar multiply at the smaller end of einsum improves\n",
        "        # XLA performance, but may introduce slight numeric differences in\n",
        "        # the Transformer attention head.\n",
        "        query = tf.multiply(query, 1.0 / math.sqrt(float(self._key_dim)))\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw\n",
        "        # attention scores.\n",
        "        attention_scores = tf.einsum(self._dot_product_equation, key, query)\n",
        "        attention_scores = self.EMA(attention_scores)\n",
        "        attention_scores = self._masked_softmax(\n",
        "            attention_scores, attention_mask\n",
        "        )\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_scores_dropout = self._dropout_layer(\n",
        "            attention_scores, training=training\n",
        "        )\n",
        "        # `context_layer` = [B, T, N, H]\n",
        "        attention_output = tf.einsum(\n",
        "            self._combine_equation, attention_scores_dropout, value\n",
        "        )\n",
        "        return attention_output, attention_scores\n",
        "\n",
        "  def get_config(self):\n",
        "        config = super(MultiHeadAttention_OVR, self).get_config()\n",
        "        config.update({\n",
        "            'EMA': self.EMA\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "zi5yfV2MMPlO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@keras.saving.register_keras_serializable()\n",
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = MultiHeadAttention_OVR(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "  def get_config(self):\n",
        "        config = super(BaseAttention, self).get_config()\n",
        "        config.update({\n",
        "            'mha': self.mha,\n",
        "            'layernorm': self.layernorm,\n",
        "            'add':self.add\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "LsoAQ9VAJwso"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@keras.saving.register_keras_serializable()\n",
        "class CrossAttention(BaseAttention):\n",
        "  def call(self, x, context):\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,\n",
        "        key=context,\n",
        "        value=context,\n",
        "        return_attention_scores=True)\n",
        "\n",
        "    # Cache the attention scores for plotting later.\n",
        "    self.last_attn_scores = attn_scores\n",
        "\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "B1QnMN3jKB-b"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@keras.saving.register_keras_serializable()\n",
        "class GlobalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "wf609eOoJ1St"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@keras.saving.register_keras_serializable()\n",
        "class CausalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "cXijxpIsKJJS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@keras.saving.register_keras_serializable()\n",
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.d_model = d_model\n",
        "    self.dff = dff\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x)\n",
        "    return x\n",
        "\n",
        "  def get_config(self):\n",
        "        config = super(FeedForward, self).get_config()\n",
        "        config.update({\n",
        "            'd_model': self.d_model,\n",
        "            'dff': self.dff,\n",
        "            'dropout_rate': self.dropout_rate,\n",
        "            'seq': self.seq,\n",
        "            'add': self.add,\n",
        "            'layer_norm': self.layer_norm\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "pIgkBJ4IUYcq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@keras.saving.register_keras_serializable()\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*,d_model, num_heads, dff, dropout_rate=0.1, decay=0.9, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.dff = dff\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.decay = decay\n",
        "\n",
        "    self.self_attention = GlobalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate,\n",
        "        decay=decay)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.self_attention(x)\n",
        "    x = self.ffn(x)\n",
        "    return x\n",
        "\n",
        "  def get_config(self):\n",
        "        config = super(EncoderLayer, self).get_config()\n",
        "        config.update({\n",
        "            'd_model': self.d_model,\n",
        "            'num_heads': self.num_heads,\n",
        "            'dff': self.dff,\n",
        "            'dropout_rate': self.dropout_rate,\n",
        "            'decay': self.decay,\n",
        "            'self_attention': self.self_attention,\n",
        "            'ffn': self.ffn\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "vkjw31mLg_H8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@keras.saving.register_keras_serializable()\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads,\n",
        "               dff, dropout_rate=0.1, decay=0.9, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    self.num_heads = num_heads\n",
        "    self.dff = dff\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.decay = decay\n",
        "\n",
        "    self.enc_layers = tf.keras.Sequential([\n",
        "        EncoderLayer(d_model=d_model,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff,\n",
        "                     dropout_rate=dropout_rate,\n",
        "                     decay=decay)\n",
        "        for _ in range(num_layers)])\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "\n",
        "    # Add dropout.\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    x = self.enc_layers(x)\n",
        "\n",
        "    return x  # Shape `(batch_size, seq_len, d_model)`.\n",
        "\n",
        "  def get_config(self):\n",
        "        config = super(Encoder, self).get_config()\n",
        "        config.update({\n",
        "            'd_model': self.d_model,\n",
        "            'num_heads': self.num_heads,\n",
        "            'dff': self.dff,\n",
        "            'dropout_rate': self.dropout_rate,\n",
        "            'decay': self.decay,\n",
        "            'num_layers': self.num_layers,\n",
        "            'enc_layers': self.enc_layers,\n",
        "            'dropout':self.dropout\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "FNvUfsPGi9ZK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@keras.saving.register_keras_serializable()\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,\n",
        "               *,\n",
        "               d_model,\n",
        "               num_heads,\n",
        "               dff,\n",
        "               dropout_rate=0.1,\n",
        "               decay=0.9,\n",
        "               **kwargs):\n",
        "    super(DecoderLayer, self).__init__(**kwargs)\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.dff = dff\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.decay = decay\n",
        "\n",
        "    self.causal_self_attention = CausalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate,\n",
        "        decay=decay)\n",
        "\n",
        "    self.cross_attention = CrossAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate,\n",
        "        decay=decay)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.causal_self_attention(x=inputs[0])\n",
        "    x = self.cross_attention(x=x, context=inputs[1])\n",
        "\n",
        "    # Cache the last attention scores for plotting later\n",
        "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
        "\n",
        "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "    return x, inputs[1]\n",
        "\n",
        "  def get_config(self):\n",
        "        config = super(DecoderLayer, self).get_config()\n",
        "        config.update({\n",
        "            'd_model': self.d_model,\n",
        "            'num_heads': self.num_heads,\n",
        "            'dff': self.dff,\n",
        "            'dropout_rate': self.dropout_rate,\n",
        "            'decay': self.decay,\n",
        "            'causal_self_attention': self.causal_self_attention,\n",
        "            'cross_attention': self.cross_attention,\n",
        "            'ffn': self.ffn\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "QzdczyLBkDT4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@keras.saving.register_keras_serializable()\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
        "               dropout_rate=0.1, decay=0.9, alpha=0.9, **kwargs):\n",
        "    super(Decoder, self).__init__(**kwargs)\n",
        "\n",
        "    self.AF = AttentionFrame(alpha=alpha)\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    self.num_heads = num_heads\n",
        "    self.dff = dff\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.decay = decay\n",
        "    self.alpha = alpha\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dec_layers = tf.keras.Sequential([\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
        "                     dff=dff, dropout_rate=dropout_rate,\n",
        "                     decay=decay)\n",
        "        for _ in range(num_layers)])\n",
        "\n",
        "  def call(self, x, context):\n",
        "    x = self.dropout(x)\n",
        "    x = self.AF(x)\n",
        "\n",
        "    x  = self.dec_layers([x, context])\n",
        "\n",
        "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
        "    return x[0]\n",
        "\n",
        "  def get_config(self):\n",
        "        config = super(Decoder, self).get_config()\n",
        "        config.update({\n",
        "            'AF': self.AF,\n",
        "            'num_layers': self.num_layers,\n",
        "            'd_model': self.d_model,\n",
        "            'num_heads': self.num_heads,\n",
        "            'dff': self.dff,\n",
        "            'dropout_rate': self.dropout_rate,\n",
        "            'decay': self.decay,\n",
        "            'alpha': self.alpha,\n",
        "            'num_layers': self.num_layers,\n",
        "            'dropout':self.dropout,\n",
        "            'dec_layers': self.dec_layers,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "H1JhYFL5lD29"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@keras.saving.register_keras_serializable()\n",
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
        "               dropout_rate=0.1, decay=0.9, alpha=0.9, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.num_layers = num_layers\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.dff = dff\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.decay = decay\n",
        "    self.alpha = alpha\n",
        "    self.pos_embedding = PositionalEmbedding(d_model=d_model, dff=dff)\n",
        "\n",
        "    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           dropout_rate=dropout_rate,\n",
        "                           decay=decay)\n",
        "\n",
        "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           dropout_rate=dropout_rate,\n",
        "                           decay=decay, alpha=alpha)\n",
        "\n",
        "  def call(self, input):\n",
        "    input = tf.cast(tf.reshape(input, shape=[-1, input.shape[-3], input.shape[-2] * input.shape[-1]]), dtype=tf.float32)\n",
        "    # To use a Keras model with `.fit` you must pass all your inputs in the\n",
        "    # first argument.\n",
        "    input = self.pos_embedding(input)\n",
        "\n",
        "    context = self.encoder(input)  # (batch_size, context_len, d_model)\n",
        "\n",
        "    output = self.decoder(input, context)  # (batch_size, target_len, d_model)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "QGWenh4iqNCI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder()"
      ],
      "metadata": {
        "id": "SGCUBbP_u8Ow"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_onehot = encoder.fit_transform(labels.reshape(-1, 1)).toarray()\n",
        "val_labels_onehot = encoder.fit_transform(val_labels.reshape(-1, 1)).toarray()"
      ],
      "metadata": {
        "id": "3aj_ggX6vKU3"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Rescaling(1./255., input_shape=(20, 100, 100)),\n",
        "        Transformer(num_layers=10, d_model=64, num_heads=8, dff=128, dropout_rate=0.2, decay=0.7, alpha=0.9),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dropout(0.6),\n",
        "        tf.keras.layers.Dense(256, activation='relu'),\n",
        "        tf.keras.layers.Dense(128, activation='gelu'),\n",
        "        tf.keras.layers.Dense(20, activation='softmax')\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "g9JEGofMwXe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(1e-5)"
      ],
      "metadata": {
        "id": "BFmFJD_AVdhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "IQFix1HJiUsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "rSL-9XBpBj8j",
        "outputId": "995debed-ebcc-4266-b61b-a8cd5b2ecc7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_32\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rescaling (Rescaling)       (None, 20, 100, 100)      0         \n",
            "                                                                 \n",
            " transformer (Transformer)   (None, 20, 64)            4981496   \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1280)              0         \n",
            "                                                                 \n",
            " dropout_22 (Dropout)        (None, 1280)              0         \n",
            "                                                                 \n",
            " dense_72 (Dense)            (None, 256)               327936    \n",
            "                                                                 \n",
            " dense_73 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_74 (Dense)            (None, 20)                2580      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5344908 (20.39 MB)\n",
            "Trainable params: 5344888 (20.39 MB)\n",
            "Non-trainable params: 20 (80.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.categories_[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tX1TMhPCtxCx",
        "outputId": "ddf46229-68d6-4bdd-c3a7-c0193dbd7cf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20,)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampling_strategy = {i : 500 for i in range(encoder.categories_[0].shape[0]) }"
      ],
      "metadata": {
        "id": "Szrle1V1uDF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_categories = np.stack([np.argmax(i) for i in labels_onehot])"
      ],
      "metadata": {
        "id": "8oSaryrHuQmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_onehot[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PLSjndvRHGP",
        "outputId": "055c25c5-c109-45da-a2ef-5ae66e0ecf7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "       0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_categories"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InQ9oMiMRB-4",
        "outputId": "149bb23f-244d-4f5c-fb09-02dead8425b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([12, 10, 16, ..., 10,  0,  6])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.categories_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5IWzoWyS58Z",
        "outputId": "a01694a8-da82-4b40-a9d5-9b5823bf9ce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array(['ban_ghe_sofa', 'ban_tho', 'cau_thang', 'chia_khoa', 'den',\n",
              "        'dien_thoai_ban', 'dong_ho', 'ke_sach', 'nha_biet_thu',\n",
              "        'nha_chung_cu', 'nha_go', 'nha_ky_tuc_xa', 'nha_lau',\n",
              "        'nha_may_ngoi', 'nha_rong', 'nha_san', 'nha_tren_cay', 'nha_tret',\n",
              "        'no_event', 'tranh_anh_treo_tuong'], dtype='<U20')]"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Chuyển đổi dữ liệu\n",
        "dataForSmote = datas.reshape(datas.shape[0], datas.shape[1] * datas.shape[2] * datas.shape[3])\n",
        "\n",
        "# Áp dụng SMOTE\n",
        "smote = SMOTE(sampling_strategy=sampling_strategy)\n",
        "x_smote, y_smote = smote.fit_resample(dataForSmote, labels_categories)\n",
        "\n",
        "x_smote = x_smote.reshape(-1 ,datas.shape[1], datas.shape[2], datas.shape[3])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0J67L3Ss68j",
        "outputId": "8cefa3fe-f345-4fee-be52-1abd9db0a576"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (500) in class 0 will be larger than the number of samples in the majority class (class #0 -> 463)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (500) in class 1 will be larger than the number of samples in the majority class (class #0 -> 463)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (500) in class 2 will be larger than the number of samples in the majority class (class #0 -> 463)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (500) in class 3 will be larger than the number of samples in the majority class (class #0 -> 463)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (500) in class 4 will be larger than the number of samples in the majority class (class #0 -> 463)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (500) in class 5 will be larger than the number of samples in the majority class (class #0 -> 463)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (500) in class 6 will be larger than the number of samples in the majority class (class #0 -> 463)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (500) in class 7 will be larger than the number of samples in the majority class (class #0 -> 463)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (500) in class 8 will be larger than the number of samples in the majority class (class #0 -> 463)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (500) in class 9 will be larger than the number of samples in the majority class (class #0 -> 463)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (500) in class 10 will be larger than the number of samples in the majority class (class #0 -> 463)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (500) in class 11 will be larger than the number of samples in the majority class (class #0 -> 463)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (500) in class 12 will be larger than the number of samples in the majority class (class #0 -> 463)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (500) in class 13 will be larger than the number of samples in the majority class (class #0 -> 463)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (500) in class 14 will be larger than the number of samples in the majority class (class #0 -> 463)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (500) in class 15 will be larger than the number of samples in the majority class (class #0 -> 463)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (500) in class 16 will be larger than the number of samples in the majority class (class #0 -> 463)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (500) in class 17 will be larger than the number of samples in the majority class (class #0 -> 463)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (500) in class 18 will be larger than the number of samples in the majority class (class #0 -> 463)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/imblearn/utils/_validation.py:313: UserWarning: After over-sampling, the number of samples (500) in class 19 will be larger than the number of samples in the majority class (class #0 -> 463)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_smote_onehot = encoder.fit_transform(y_smote.reshape(-1, 1)).toarray()"
      ],
      "metadata": {
        "id": "9Ct7bJaitMnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callback = tf.keras.callbacks.ModelCheckpoint('/content/drive/MyDrive/model_AF.keras', monitor='val_accuracy', verbose=2, save_best_only=True)"
      ],
      "metadata": {
        "id": "p-GaZhL-0qow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x=x_smote, y=y_smote_onehot, batch_size=64, epochs=100, validation_data=[val_datas, val_labels_onehot], callbacks=[callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASM-WP6KkAPI",
        "outputId": "524df4ce-f47f-45d7-f712-edb569cc1129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 3.4247 - accuracy: 0.0521\n",
            "Epoch 1: val_accuracy improved from -inf to 0.05098, saving model to /content/drive/MyDrive/model_AF.keras\n",
            "157/157 [==============================] - 261s 842ms/step - loss: 3.4247 - accuracy: 0.0521 - val_loss: 3.0311 - val_accuracy: 0.0510\n",
            "Epoch 2/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 3.2169 - accuracy: 0.0540\n",
            "Epoch 2: val_accuracy did not improve from 0.05098\n",
            "157/157 [==============================] - 114s 727ms/step - loss: 3.2169 - accuracy: 0.0540 - val_loss: 3.0139 - val_accuracy: 0.0475\n",
            "Epoch 3/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 3.1327 - accuracy: 0.0629\n",
            "Epoch 3: val_accuracy improved from 0.05098 to 0.15750, saving model to /content/drive/MyDrive/model_AF.keras\n",
            "157/157 [==============================] - 119s 758ms/step - loss: 3.1327 - accuracy: 0.0629 - val_loss: 2.8221 - val_accuracy: 0.1575\n",
            "Epoch 4/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 2.9922 - accuracy: 0.0903\n",
            "Epoch 4: val_accuracy improved from 0.15750 to 0.20747, saving model to /content/drive/MyDrive/model_AF.keras\n",
            "157/157 [==============================] - 116s 738ms/step - loss: 2.9922 - accuracy: 0.0903 - val_loss: 2.6919 - val_accuracy: 0.2075\n",
            "Epoch 5/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 2.8597 - accuracy: 0.1333\n",
            "Epoch 5: val_accuracy improved from 0.20747 to 0.32458, saving model to /content/drive/MyDrive/model_AF.keras\n",
            "157/157 [==============================] - 116s 742ms/step - loss: 2.8597 - accuracy: 0.1333 - val_loss: 2.4957 - val_accuracy: 0.3246\n",
            "Epoch 6/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 2.7167 - accuracy: 0.1834\n",
            "Epoch 6: val_accuracy did not improve from 0.32458\n",
            "157/157 [==============================] - 114s 729ms/step - loss: 2.7167 - accuracy: 0.1834 - val_loss: 2.3165 - val_accuracy: 0.2650\n",
            "Epoch 7/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 2.5718 - accuracy: 0.2199\n",
            "Epoch 7: val_accuracy improved from 0.32458 to 0.39223, saving model to /content/drive/MyDrive/model_AF.keras\n",
            "157/157 [==============================] - 117s 743ms/step - loss: 2.5718 - accuracy: 0.2199 - val_loss: 2.1264 - val_accuracy: 0.3922\n",
            "Epoch 8/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 2.4535 - accuracy: 0.2524\n",
            "Epoch 8: val_accuracy improved from 0.39223 to 0.41242, saving model to /content/drive/MyDrive/model_AF.keras\n",
            "157/157 [==============================] - 117s 743ms/step - loss: 2.4535 - accuracy: 0.2524 - val_loss: 1.8987 - val_accuracy: 0.4124\n",
            "Epoch 9/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 2.3078 - accuracy: 0.2942\n",
            "Epoch 9: val_accuracy improved from 0.41242 to 0.50883, saving model to /content/drive/MyDrive/model_AF.keras\n",
            "157/157 [==============================] - 116s 740ms/step - loss: 2.3078 - accuracy: 0.2942 - val_loss: 1.7236 - val_accuracy: 0.5088\n",
            "Epoch 10/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 2.1622 - accuracy: 0.3262\n",
            "Epoch 10: val_accuracy improved from 0.50883 to 0.51388, saving model to /content/drive/MyDrive/model_AF.keras\n",
            "157/157 [==============================] - 116s 739ms/step - loss: 2.1622 - accuracy: 0.3262 - val_loss: 1.5423 - val_accuracy: 0.5139\n",
            "Epoch 11/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 2.0319 - accuracy: 0.3659\n",
            "Epoch 11: val_accuracy improved from 0.51388 to 0.59364, saving model to /content/drive/MyDrive/model_AF.keras\n",
            "157/157 [==============================] - 117s 742ms/step - loss: 2.0319 - accuracy: 0.3659 - val_loss: 1.4067 - val_accuracy: 0.5936\n",
            "Epoch 12/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.9175 - accuracy: 0.4071\n",
            "Epoch 12: val_accuracy improved from 0.59364 to 0.62797, saving model to /content/drive/MyDrive/model_AF.keras\n",
            "157/157 [==============================] - 117s 743ms/step - loss: 1.9175 - accuracy: 0.4071 - val_loss: 1.2673 - val_accuracy: 0.6280\n",
            "Epoch 13/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.8286 - accuracy: 0.4297\n",
            "Epoch 13: val_accuracy improved from 0.62797 to 0.65623, saving model to /content/drive/MyDrive/model_AF.keras\n",
            "157/157 [==============================] - 116s 741ms/step - loss: 1.8286 - accuracy: 0.4297 - val_loss: 1.1455 - val_accuracy: 0.6562\n",
            "Epoch 14/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.7066 - accuracy: 0.4612\n",
            "Epoch 14: val_accuracy improved from 0.65623 to 0.65724, saving model to /content/drive/MyDrive/model_AF.keras\n",
            "157/157 [==============================] - 116s 742ms/step - loss: 1.7066 - accuracy: 0.4612 - val_loss: 1.1332 - val_accuracy: 0.6572\n",
            "Epoch 15/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.6130 - accuracy: 0.4941\n",
            "Epoch 15: val_accuracy improved from 0.65724 to 0.69510, saving model to /content/drive/MyDrive/model_AF.keras\n",
            "157/157 [==============================] - 116s 741ms/step - loss: 1.6130 - accuracy: 0.4941 - val_loss: 0.9317 - val_accuracy: 0.6951\n",
            "Epoch 16/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.5228 - accuracy: 0.5238\n",
            "Epoch 16: val_accuracy improved from 0.69510 to 0.70621, saving model to /content/drive/MyDrive/model_AF.keras\n",
            "157/157 [==============================] - 116s 741ms/step - loss: 1.5228 - accuracy: 0.5238 - val_loss: 0.9464 - val_accuracy: 0.7062\n",
            "Epoch 17/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.4490 - accuracy: 0.5455\n",
            "Epoch 17: val_accuracy improved from 0.70621 to 0.71933, saving model to /content/drive/MyDrive/model_AF.keras\n",
            "157/157 [==============================] - 116s 740ms/step - loss: 1.4490 - accuracy: 0.5455 - val_loss: 0.8776 - val_accuracy: 0.7193\n",
            "Epoch 18/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.3942 - accuracy: 0.5623\n",
            "Epoch 18: val_accuracy improved from 0.71933 to 0.75517, saving model to /content/drive/MyDrive/model_AF.keras\n",
            "157/157 [==============================] - 116s 741ms/step - loss: 1.3942 - accuracy: 0.5623 - val_loss: 0.8215 - val_accuracy: 0.7552\n",
            "Epoch 19/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.3138 - accuracy: 0.5857\n",
            "Epoch 19: val_accuracy improved from 0.75517 to 0.76729, saving model to /content/drive/MyDrive/model_AF.keras\n",
            "157/157 [==============================] - 117s 747ms/step - loss: 1.3138 - accuracy: 0.5857 - val_loss: 0.7110 - val_accuracy: 0.7673\n",
            "Epoch 20/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.2645 - accuracy: 0.6007\n",
            "Epoch 20: val_accuracy did not improve from 0.76729\n",
            "157/157 [==============================] - 118s 750ms/step - loss: 1.2645 - accuracy: 0.6007 - val_loss: 0.7512 - val_accuracy: 0.7658\n",
            "Epoch 21/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.2148 - accuracy: 0.6146\n",
            "Epoch 21: val_accuracy improved from 0.76729 to 0.79253, saving model to /content/drive/MyDrive/model_AF.keras\n",
            "157/157 [==============================] - 120s 764ms/step - loss: 1.2148 - accuracy: 0.6146 - val_loss: 0.6352 - val_accuracy: 0.7925\n",
            "Epoch 22/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.1558 - accuracy: 0.6422\n",
            "Epoch 22: val_accuracy improved from 0.79253 to 0.80565, saving model to /content/drive/MyDrive/model_AF.keras\n",
            "157/157 [==============================] - 117s 748ms/step - loss: 1.1558 - accuracy: 0.6422 - val_loss: 0.6216 - val_accuracy: 0.8057\n",
            "Epoch 23/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.1170 - accuracy: 0.6498\n",
            "Epoch 23: val_accuracy improved from 0.80565 to 0.82837, saving model to /content/drive/MyDrive/model_AF.keras\n",
            "157/157 [==============================] - 116s 737ms/step - loss: 1.1170 - accuracy: 0.6498 - val_loss: 0.5563 - val_accuracy: 0.8284\n",
            "Epoch 24/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.0812 - accuracy: 0.6602\n",
            "Epoch 24: val_accuracy improved from 0.82837 to 0.84351, saving model to /content/drive/MyDrive/model_AF.keras\n",
            "157/157 [==============================] - 117s 744ms/step - loss: 1.0812 - accuracy: 0.6602 - val_loss: 0.5044 - val_accuracy: 0.8435\n",
            "Epoch 25/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.0437 - accuracy: 0.6705\n",
            "Epoch 25: val_accuracy improved from 0.84351 to 0.86724, saving model to /content/drive/MyDrive/model_AF.keras\n",
            "157/157 [==============================] - 118s 749ms/step - loss: 1.0437 - accuracy: 0.6705 - val_loss: 0.4553 - val_accuracy: 0.8672\n",
            "Epoch 26/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.0184 - accuracy: 0.6804\n",
            "Epoch 26: val_accuracy did not improve from 0.86724\n",
            "157/157 [==============================] - 117s 743ms/step - loss: 1.0184 - accuracy: 0.6804 - val_loss: 0.5072 - val_accuracy: 0.8299\n",
            "Epoch 27/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.9717 - accuracy: 0.6936\n",
            "Epoch 27: val_accuracy did not improve from 0.86724\n",
            "157/157 [==============================] - 114s 728ms/step - loss: 0.9717 - accuracy: 0.6936 - val_loss: 0.4694 - val_accuracy: 0.8642\n",
            "Epoch 28/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.9366 - accuracy: 0.7034\n",
            "Epoch 28: val_accuracy improved from 0.86724 to 0.88137, saving model to /content/drive/MyDrive/model_AF.keras\n",
            "157/157 [==============================] - 116s 739ms/step - loss: 0.9366 - accuracy: 0.7034 - val_loss: 0.4171 - val_accuracy: 0.8814\n",
            "Epoch 29/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.9152 - accuracy: 0.7142\n",
            "Epoch 29: val_accuracy did not improve from 0.88137\n",
            "157/157 [==============================] - 114s 726ms/step - loss: 0.9152 - accuracy: 0.7142 - val_loss: 0.4729 - val_accuracy: 0.8536\n",
            "Epoch 30/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.8745 - accuracy: 0.7178\n",
            "Epoch 30: val_accuracy improved from 0.88137 to 0.88541, saving model to /content/drive/MyDrive/model_AF.keras\n",
            "157/157 [==============================] - 116s 740ms/step - loss: 0.8745 - accuracy: 0.7178 - val_loss: 0.4138 - val_accuracy: 0.8854\n",
            "Epoch 31/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.8506 - accuracy: 0.7292\n",
            "Epoch 31: val_accuracy did not improve from 0.88541\n",
            "157/157 [==============================] - 114s 727ms/step - loss: 0.8506 - accuracy: 0.7292 - val_loss: 0.4527 - val_accuracy: 0.8617\n",
            "Epoch 32/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.8124 - accuracy: 0.7415\n",
            "Epoch 32: val_accuracy did not improve from 0.88541\n",
            "157/157 [==============================] - 114s 725ms/step - loss: 0.8124 - accuracy: 0.7415 - val_loss: 0.4227 - val_accuracy: 0.8834\n",
            "Epoch 33/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.7944 - accuracy: 0.7502\n",
            "Epoch 33: val_accuracy improved from 0.88541 to 0.89551, saving model to /content/drive/MyDrive/model_AF.keras\n",
            "157/157 [==============================] - 116s 739ms/step - loss: 0.7944 - accuracy: 0.7502 - val_loss: 0.3922 - val_accuracy: 0.8955\n",
            "Epoch 34/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.7867 - accuracy: 0.7550\n",
            "Epoch 34: val_accuracy did not improve from 0.89551\n",
            "157/157 [==============================] - 114s 726ms/step - loss: 0.7867 - accuracy: 0.7550 - val_loss: 0.3977 - val_accuracy: 0.8955\n",
            "Epoch 35/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.7648 - accuracy: 0.7575\n",
            "Epoch 35: val_accuracy did not improve from 0.89551\n",
            "157/157 [==============================] - 114s 727ms/step - loss: 0.7648 - accuracy: 0.7575 - val_loss: 0.3987 - val_accuracy: 0.8849\n",
            "Epoch 36/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.7397 - accuracy: 0.7657\n",
            "Epoch 36: val_accuracy did not improve from 0.89551\n",
            "157/157 [==============================] - 114s 725ms/step - loss: 0.7397 - accuracy: 0.7657 - val_loss: 0.4525 - val_accuracy: 0.8824\n",
            "Epoch 37/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.7140 - accuracy: 0.7757\n",
            "Epoch 37: val_accuracy improved from 0.89551 to 0.90510, saving model to /content/drive/MyDrive/model_AF.keras\n",
            "157/157 [==============================] - 116s 741ms/step - loss: 0.7140 - accuracy: 0.7757 - val_loss: 0.3390 - val_accuracy: 0.9051\n",
            "Epoch 38/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.6908 - accuracy: 0.7831\n",
            "Epoch 38: val_accuracy did not improve from 0.90510\n",
            "157/157 [==============================] - 114s 725ms/step - loss: 0.6908 - accuracy: 0.7831 - val_loss: 0.3943 - val_accuracy: 0.8985\n",
            "Epoch 39/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.6866 - accuracy: 0.7832\n",
            "Epoch 39: val_accuracy improved from 0.90510 to 0.90560, saving model to /content/drive/MyDrive/model_AF.keras\n",
            "157/157 [==============================] - 116s 741ms/step - loss: 0.6866 - accuracy: 0.7832 - val_loss: 0.3533 - val_accuracy: 0.9056\n",
            "Epoch 40/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.6656 - accuracy: 0.7927\n",
            "Epoch 40: val_accuracy improved from 0.90560 to 0.90863, saving model to /content/drive/MyDrive/model_AF.keras\n",
            "157/157 [==============================] - 119s 755ms/step - loss: 0.6656 - accuracy: 0.7927 - val_loss: 0.3255 - val_accuracy: 0.9086\n",
            "Epoch 41/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.6369 - accuracy: 0.7997\n",
            "Epoch 41: val_accuracy improved from 0.90863 to 0.91418, saving model to /content/drive/MyDrive/model_AF.keras\n",
            "157/157 [==============================] - 116s 742ms/step - loss: 0.6369 - accuracy: 0.7997 - val_loss: 0.3550 - val_accuracy: 0.9142\n",
            "Epoch 42/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.6252 - accuracy: 0.8045\n",
            "Epoch 42: val_accuracy improved from 0.91418 to 0.92226, saving model to /content/drive/MyDrive/model_AF.keras\n",
            "157/157 [==============================] - 116s 739ms/step - loss: 0.6252 - accuracy: 0.8045 - val_loss: 0.3343 - val_accuracy: 0.9223\n",
            "Epoch 43/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.6144 - accuracy: 0.8050\n",
            "Epoch 43: val_accuracy did not improve from 0.92226\n",
            "157/157 [==============================] - 115s 730ms/step - loss: 0.6144 - accuracy: 0.8050 - val_loss: 0.3236 - val_accuracy: 0.9167\n",
            "Epoch 44/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.5803 - accuracy: 0.8142\n",
            "Epoch 44: val_accuracy did not improve from 0.92226\n",
            "157/157 [==============================] - 114s 728ms/step - loss: 0.5803 - accuracy: 0.8142 - val_loss: 0.4158 - val_accuracy: 0.8985\n",
            "Epoch 45/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.5800 - accuracy: 0.8143\n",
            "Epoch 45: val_accuracy did not improve from 0.92226\n",
            "157/157 [==============================] - 114s 725ms/step - loss: 0.5800 - accuracy: 0.8143 - val_loss: 0.3795 - val_accuracy: 0.9046\n",
            "Epoch 46/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.5652 - accuracy: 0.8225\n",
            "Epoch 46: val_accuracy did not improve from 0.92226\n",
            "157/157 [==============================] - 114s 725ms/step - loss: 0.5652 - accuracy: 0.8225 - val_loss: 0.3952 - val_accuracy: 0.9091\n",
            "Epoch 47/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.5414 - accuracy: 0.8242\n",
            "Epoch 47: val_accuracy did not improve from 0.92226\n",
            "157/157 [==============================] - 115s 732ms/step - loss: 0.5414 - accuracy: 0.8242 - val_loss: 0.3219 - val_accuracy: 0.9101\n",
            "Epoch 48/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.5230 - accuracy: 0.8337\n",
            "Epoch 48: val_accuracy did not improve from 0.92226\n",
            "157/157 [==============================] - 114s 729ms/step - loss: 0.5230 - accuracy: 0.8337 - val_loss: 0.3547 - val_accuracy: 0.9001\n",
            "Epoch 49/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.5210 - accuracy: 0.8333\n",
            "Epoch 49: val_accuracy did not improve from 0.92226\n",
            "157/157 [==============================] - 114s 728ms/step - loss: 0.5210 - accuracy: 0.8333 - val_loss: 0.3465 - val_accuracy: 0.9202\n",
            "Epoch 50/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.5187 - accuracy: 0.8362\n",
            "Epoch 50: val_accuracy did not improve from 0.92226\n",
            "157/157 [==============================] - 114s 727ms/step - loss: 0.5187 - accuracy: 0.8362 - val_loss: 0.3696 - val_accuracy: 0.9061\n",
            "Epoch 51/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.4902 - accuracy: 0.8456\n",
            "Epoch 51: val_accuracy did not improve from 0.92226\n",
            "157/157 [==============================] - 114s 729ms/step - loss: 0.4902 - accuracy: 0.8456 - val_loss: 0.3071 - val_accuracy: 0.9172\n",
            "Epoch 52/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.4658 - accuracy: 0.8506\n",
            "Epoch 52: val_accuracy did not improve from 0.92226\n",
            "157/157 [==============================] - 114s 726ms/step - loss: 0.4658 - accuracy: 0.8506 - val_loss: 0.3573 - val_accuracy: 0.9187\n",
            "Epoch 53/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.4708 - accuracy: 0.8514\n",
            "Epoch 53: val_accuracy did not improve from 0.92226\n",
            "157/157 [==============================] - 114s 729ms/step - loss: 0.4708 - accuracy: 0.8514 - val_loss: 0.3200 - val_accuracy: 0.9218\n",
            "Epoch 54/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.4462 - accuracy: 0.8605\n",
            "Epoch 54: val_accuracy did not improve from 0.92226\n",
            "157/157 [==============================] - 114s 725ms/step - loss: 0.4462 - accuracy: 0.8605 - val_loss: 0.3012 - val_accuracy: 0.9187\n",
            "Epoch 55/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.4562 - accuracy: 0.8537\n",
            "Epoch 55: val_accuracy did not improve from 0.92226\n",
            "157/157 [==============================] - 114s 728ms/step - loss: 0.4562 - accuracy: 0.8537 - val_loss: 0.3506 - val_accuracy: 0.9076\n",
            "Epoch 56/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.4266 - accuracy: 0.8635\n",
            "Epoch 56: val_accuracy improved from 0.92226 to 0.93640, saving model to /content/drive/MyDrive/model_AF.keras\n",
            "157/157 [==============================] - 116s 741ms/step - loss: 0.4266 - accuracy: 0.8635 - val_loss: 0.2893 - val_accuracy: 0.9364\n",
            "Epoch 57/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.4302 - accuracy: 0.8636\n",
            "Epoch 57: val_accuracy did not improve from 0.93640\n",
            "157/157 [==============================] - 117s 745ms/step - loss: 0.4302 - accuracy: 0.8636 - val_loss: 0.3018 - val_accuracy: 0.9334\n",
            "Epoch 58/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.4181 - accuracy: 0.8647\n",
            "Epoch 58: val_accuracy did not improve from 0.93640\n",
            "157/157 [==============================] - 114s 728ms/step - loss: 0.4181 - accuracy: 0.8647 - val_loss: 0.2755 - val_accuracy: 0.9293\n",
            "Epoch 59/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.4069 - accuracy: 0.8729\n",
            "Epoch 59: val_accuracy did not improve from 0.93640\n",
            "157/157 [==============================] - 114s 729ms/step - loss: 0.4069 - accuracy: 0.8729 - val_loss: 0.3132 - val_accuracy: 0.9319\n",
            "Epoch 60/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.3858 - accuracy: 0.8746\n",
            "Epoch 60: val_accuracy did not improve from 0.93640\n",
            "157/157 [==============================] - 114s 727ms/step - loss: 0.3858 - accuracy: 0.8746 - val_loss: 0.3000 - val_accuracy: 0.9187\n",
            "Epoch 61/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.3790 - accuracy: 0.8799\n",
            "Epoch 61: val_accuracy did not improve from 0.93640\n",
            "157/157 [==============================] - 114s 727ms/step - loss: 0.3790 - accuracy: 0.8799 - val_loss: 0.3316 - val_accuracy: 0.9213\n",
            "Epoch 62/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.3983 - accuracy: 0.8729\n",
            "Epoch 62: val_accuracy did not improve from 0.93640\n",
            "157/157 [==============================] - 114s 727ms/step - loss: 0.3983 - accuracy: 0.8729 - val_loss: 0.3405 - val_accuracy: 0.9238\n",
            "Epoch 63/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.3764 - accuracy: 0.8789\n",
            "Epoch 63: val_accuracy did not improve from 0.93640\n",
            "157/157 [==============================] - 115s 732ms/step - loss: 0.3764 - accuracy: 0.8789 - val_loss: 0.3296 - val_accuracy: 0.9308\n",
            "Epoch 64/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.3691 - accuracy: 0.8805\n",
            "Epoch 64: val_accuracy did not improve from 0.93640\n",
            "157/157 [==============================] - 114s 726ms/step - loss: 0.3691 - accuracy: 0.8805 - val_loss: 0.2680 - val_accuracy: 0.9349\n",
            "Epoch 65/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.3456 - accuracy: 0.8903\n",
            "Epoch 65: val_accuracy did not improve from 0.93640\n",
            "157/157 [==============================] - 115s 730ms/step - loss: 0.3456 - accuracy: 0.8903 - val_loss: 0.3984 - val_accuracy: 0.9258\n",
            "Epoch 66/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.3375 - accuracy: 0.8932\n",
            "Epoch 66: val_accuracy did not improve from 0.93640\n",
            "157/157 [==============================] - 114s 727ms/step - loss: 0.3375 - accuracy: 0.8932 - val_loss: 0.2940 - val_accuracy: 0.9248\n",
            "Epoch 67/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.3330 - accuracy: 0.8924\n",
            "Epoch 67: val_accuracy did not improve from 0.93640\n",
            "157/157 [==============================] - 114s 727ms/step - loss: 0.3330 - accuracy: 0.8924 - val_loss: 0.2964 - val_accuracy: 0.9278\n",
            "Epoch 68/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.3167 - accuracy: 0.8976\n",
            "Epoch 68: val_accuracy did not improve from 0.93640\n",
            "157/157 [==============================] - 113s 722ms/step - loss: 0.3167 - accuracy: 0.8976 - val_loss: 0.3016 - val_accuracy: 0.9354\n",
            "Epoch 69/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.3256 - accuracy: 0.8949\n",
            "Epoch 69: val_accuracy did not improve from 0.93640\n",
            "157/157 [==============================] - 114s 725ms/step - loss: 0.3256 - accuracy: 0.8949 - val_loss: 0.3697 - val_accuracy: 0.9283\n",
            "Epoch 70/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.3077 - accuracy: 0.9021\n",
            "Epoch 70: val_accuracy did not improve from 0.93640\n",
            "157/157 [==============================] - 113s 721ms/step - loss: 0.3077 - accuracy: 0.9021 - val_loss: 0.3390 - val_accuracy: 0.9319\n",
            "Epoch 71/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.2981 - accuracy: 0.9057\n",
            "Epoch 71: val_accuracy did not improve from 0.93640\n",
            "157/157 [==============================] - 113s 722ms/step - loss: 0.2981 - accuracy: 0.9057 - val_loss: 0.3414 - val_accuracy: 0.9278\n",
            "Epoch 72/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.2906 - accuracy: 0.9070\n",
            "Epoch 72: val_accuracy did not improve from 0.93640\n",
            "157/157 [==============================] - 113s 720ms/step - loss: 0.2906 - accuracy: 0.9070 - val_loss: 0.3286 - val_accuracy: 0.9283\n",
            "Epoch 73/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.2820 - accuracy: 0.9088\n",
            "Epoch 73: val_accuracy did not improve from 0.93640\n",
            "157/157 [==============================] - 115s 732ms/step - loss: 0.2820 - accuracy: 0.9088 - val_loss: 0.3240 - val_accuracy: 0.9364\n",
            "Epoch 74/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.2805 - accuracy: 0.9088\n",
            "Epoch 74: val_accuracy did not improve from 0.93640\n",
            "157/157 [==============================] - 115s 734ms/step - loss: 0.2805 - accuracy: 0.9088 - val_loss: 0.3420 - val_accuracy: 0.9293\n",
            "Epoch 75/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.2885 - accuracy: 0.9064\n",
            "Epoch 75: val_accuracy did not improve from 0.93640\n",
            "157/157 [==============================] - 114s 728ms/step - loss: 0.2885 - accuracy: 0.9064 - val_loss: 0.2904 - val_accuracy: 0.9329\n",
            "Epoch 76/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.2725 - accuracy: 0.9126\n",
            "Epoch 76: val_accuracy did not improve from 0.93640\n",
            "157/157 [==============================] - 115s 730ms/step - loss: 0.2725 - accuracy: 0.9126 - val_loss: 0.3044 - val_accuracy: 0.9243\n",
            "Epoch 77/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.2532 - accuracy: 0.9212\n",
            "Epoch 77: val_accuracy did not improve from 0.93640\n",
            "157/157 [==============================] - 114s 729ms/step - loss: 0.2532 - accuracy: 0.9212 - val_loss: 0.3634 - val_accuracy: 0.9324\n",
            "Epoch 78/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.2615 - accuracy: 0.9175\n",
            "Epoch 78: val_accuracy improved from 0.93640 to 0.93892, saving model to /content/drive/MyDrive/model_AF.keras\n",
            "157/157 [==============================] - 115s 736ms/step - loss: 0.2615 - accuracy: 0.9175 - val_loss: 0.3165 - val_accuracy: 0.9389\n",
            "Epoch 79/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.2576 - accuracy: 0.9162\n",
            "Epoch 79: val_accuracy did not improve from 0.93892\n",
            "157/157 [==============================] - 113s 721ms/step - loss: 0.2576 - accuracy: 0.9162 - val_loss: 0.3301 - val_accuracy: 0.9258\n",
            "Epoch 80/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.2385 - accuracy: 0.9211\n",
            "Epoch 80: val_accuracy did not improve from 0.93892\n",
            "157/157 [==============================] - 114s 726ms/step - loss: 0.2385 - accuracy: 0.9211 - val_loss: 0.3268 - val_accuracy: 0.9379\n",
            "Epoch 81/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.2424 - accuracy: 0.9232\n",
            "Epoch 81: val_accuracy did not improve from 0.93892\n",
            "157/157 [==============================] - 113s 721ms/step - loss: 0.2424 - accuracy: 0.9232 - val_loss: 0.3290 - val_accuracy: 0.9349\n",
            "Epoch 82/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.2193 - accuracy: 0.9270\n",
            "Epoch 82: val_accuracy did not improve from 0.93892\n",
            "157/157 [==============================] - 114s 724ms/step - loss: 0.2193 - accuracy: 0.9270 - val_loss: 0.3336 - val_accuracy: 0.9359\n",
            "Epoch 83/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.2306 - accuracy: 0.9290\n",
            "Epoch 83: val_accuracy did not improve from 0.93892\n",
            "157/157 [==============================] - 113s 723ms/step - loss: 0.2306 - accuracy: 0.9290 - val_loss: 0.3383 - val_accuracy: 0.9354\n",
            "Epoch 84/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.2275 - accuracy: 0.9248\n",
            "Epoch 84: val_accuracy did not improve from 0.93892\n",
            "157/157 [==============================] - 114s 726ms/step - loss: 0.2275 - accuracy: 0.9248 - val_loss: 0.3562 - val_accuracy: 0.9288\n",
            "Epoch 85/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.2251 - accuracy: 0.9296\n",
            "Epoch 85: val_accuracy did not improve from 0.93892\n",
            "157/157 [==============================] - 113s 721ms/step - loss: 0.2251 - accuracy: 0.9296 - val_loss: 0.3081 - val_accuracy: 0.9369\n",
            "Epoch 86/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.2073 - accuracy: 0.9337\n",
            "Epoch 86: val_accuracy did not improve from 0.93892\n",
            "157/157 [==============================] - 114s 723ms/step - loss: 0.2073 - accuracy: 0.9337 - val_loss: 0.3303 - val_accuracy: 0.9369\n",
            "Epoch 87/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.1935 - accuracy: 0.9398\n",
            "Epoch 87: val_accuracy improved from 0.93892 to 0.94245, saving model to /content/drive/MyDrive/model_AF.keras\n",
            "157/157 [==============================] - 115s 733ms/step - loss: 0.1935 - accuracy: 0.9398 - val_loss: 0.3103 - val_accuracy: 0.9425\n",
            "Epoch 88/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.2151 - accuracy: 0.9305\n",
            "Epoch 88: val_accuracy did not improve from 0.94245\n",
            "157/157 [==============================] - 113s 720ms/step - loss: 0.2151 - accuracy: 0.9305 - val_loss: 0.2794 - val_accuracy: 0.9419\n",
            "Epoch 89/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.1898 - accuracy: 0.9376\n",
            "Epoch 89: val_accuracy did not improve from 0.94245\n",
            "157/157 [==============================] - 113s 721ms/step - loss: 0.1898 - accuracy: 0.9376 - val_loss: 0.3641 - val_accuracy: 0.9283\n",
            "Epoch 90/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.1941 - accuracy: 0.9368\n",
            "Epoch 90: val_accuracy did not improve from 0.94245\n",
            "157/157 [==============================] - 113s 722ms/step - loss: 0.1941 - accuracy: 0.9368 - val_loss: 0.3672 - val_accuracy: 0.9313\n",
            "Epoch 91/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.1750 - accuracy: 0.9436\n",
            "Epoch 91: val_accuracy did not improve from 0.94245\n",
            "157/157 [==============================] - 113s 723ms/step - loss: 0.1750 - accuracy: 0.9436 - val_loss: 0.3370 - val_accuracy: 0.9319\n",
            "Epoch 92/100\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.1789 - accuracy: 0.9409\n",
            "Epoch 92: val_accuracy did not improve from 0.94245\n",
            "157/157 [==============================] - 113s 720ms/step - loss: 0.1789 - accuracy: 0.9409 - val_loss: 0.3239 - val_accuracy: 0.9369\n",
            "Epoch 93/100\n",
            " 28/157 [====>.........................] - ETA: 1:31 - loss: 0.1873 - accuracy: 0.9381"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model('/content/drive/MyDrive/model_AF1.keras')"
      ],
      "metadata": {
        "id": "VC68dhUP0uwU"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(val_datas)"
      ],
      "metadata": {
        "id": "c13PoRCRqe8h",
        "outputId": "205f5e98-996e-49b2-a14c-f3bfb2d931cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "62/62 [==============================] - 18s 93ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lb = np.argmax(val_labels_onehot, axis=1)"
      ],
      "metadata": {
        "id": "5p_0e4vUqybk"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "iEr3xMEnq5QH"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(lb, np.argmax(y_pred, axis=1)))"
      ],
      "metadata": {
        "id": "Y9rJF4WirIlV",
        "outputId": "35efd7de-1e41-43f5-c8d2-bc8c4bccdfd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      0.99       310\n",
            "           1       1.00      0.97      0.98       218\n",
            "           2       0.98      0.98      0.98        62\n",
            "           3       0.95      0.97      0.96        64\n",
            "           4       1.00      0.95      0.98        62\n",
            "           5       0.99      1.00      0.99        72\n",
            "           6       0.95      1.00      0.97        56\n",
            "           7       0.97      1.00      0.98       195\n",
            "           8       0.99      0.96      0.98       102\n",
            "           9       0.97      1.00      0.98        91\n",
            "          10       1.00      0.98      0.99       112\n",
            "          11       0.98      0.98      0.98        66\n",
            "          12       0.93      1.00      0.96        50\n",
            "          13       0.95      0.93      0.94        56\n",
            "          14       0.91      0.91      0.91        46\n",
            "          15       0.89      0.95      0.92        59\n",
            "          16       0.97      0.96      0.96        70\n",
            "          17       0.94      0.96      0.95        47\n",
            "          18       1.00      0.82      0.90        17\n",
            "          19       1.00      1.00      1.00       226\n",
            "\n",
            "    accuracy                           0.98      1981\n",
            "   macro avg       0.97      0.97      0.97      1981\n",
            "weighted avg       0.98      0.98      0.98      1981\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mO4AvImfrOzh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}