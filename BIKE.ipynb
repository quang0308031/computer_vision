{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YTtZPMB5fmY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import cv2 as cv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = np.load('/content/drive/MyDrive/final_data/dataset.npy')\n",
        "label_train = np.load('/content/drive/MyDrive/final_data/labels.npy')\n",
        "\n",
        "data_val = np.load('/content/drive/MyDrive/final_data/val_dataset.npy')\n",
        "label_val = np.load('/content/drive/MyDrive/final_data/val_labels.npy')"
      ],
      "metadata": {
        "id": "03mTdL3o5r6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_names = np.unique(label_train)"
      ],
      "metadata": {
        "id": "P1w01eLp5yyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in unique_names:\n",
        "  os.makedirs(name = '/content/data/' + i, exist_ok = True)\n",
        "  idxs = np.where(label_train == i)\n",
        "  count = 0\n",
        "  for j in data_train[idxs]:\n",
        "    np.save('/content/data/' + i + '/' + str(count), j)\n",
        "    count += 1"
      ],
      "metadata": {
        "id": "i73Q8IOw51vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "edr = LabelEncoder()"
      ],
      "metadata": {
        "id": "g5bGs52M54dB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_trainedr = edr.fit_transform(label_train)"
      ],
      "metadata": {
        "id": "s8A1jsmK56tT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('bodyl_train.txt', mode='w') as f:\n",
        "\n",
        "  for i in range(len(edr.classes_)):\n",
        "    list_video = os.listdir('/content/data/' + edr.classes_[i])\n",
        "    for j in list_video:\n",
        "      f.write(edr.classes_[i] + '/' + j + ' 20 ' + str(i) + '\\n')\n",
        "\n",
        "  f.close()"
      ],
      "metadata": {
        "id": "hEvUXazA57_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in unique_names:\n",
        "  os.makedirs(name = '/content/data_BIKE_val/' + i, exist_ok = True)\n",
        "  idxs = np.where(label_val == i)\n",
        "  count = 0\n",
        "  for j in data_val[idxs]:\n",
        "    np.save('/content/data_BIKE_val/' + i + '/' + str(count), j)\n",
        "    count += 1"
      ],
      "metadata": {
        "id": "lZYX5cwZ59y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('bodyl_val.txt', mode='w') as f:\n",
        "\n",
        "  for i in range(len(edr.classes_)):\n",
        "    list_video = os.listdir('/content/data_BIKE_val/' + edr.classes_[i])\n",
        "    for j in list_video:\n",
        "      f.write(edr.classes_[i] + '/' + j + ' 20 ' + str(i) + '\\n')\n",
        "\n",
        "  f.close()"
      ],
      "metadata": {
        "id": "ZQ0xqV4w5_hH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame({'id' : np.arange(len(edr.classes_)), 'name' : edr.classes_}).set_index('id')"
      ],
      "metadata": {
        "id": "ooMiyvuekfvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('/content/labels.csv')"
      ],
      "metadata": {
        "id": "4OGX6DPpkf14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchnet\n",
        "!pip install ftfy\n",
        "!pip install dotmap\n",
        "!pip install torch torchvision\n",
        "!pip install decord"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLOs2NZN6Q6w",
        "outputId": "1917de26-ecfc-4880-b42c-46e06f8f3afc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchnet\n",
            "  Downloading torchnet-0.0.4.tar.gz (23 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchnet) (2.2.1+cu121)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchnet) (1.16.0)\n",
            "Collecting visdom (from torchnet)\n",
            "  Downloading visdom-0.2.4.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->torchnet)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->torchnet)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->torchnet)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch->torchnet)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m707.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch->torchnet)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch->torchnet)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch->torchnet)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch->torchnet)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch->torchnet)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch->torchnet)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch->torchnet)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->torchnet)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.10/dist-packages (from visdom->torchnet) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from visdom->torchnet) (1.11.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from visdom->torchnet) (2.31.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from visdom->torchnet) (6.3.3)\n",
            "Collecting jsonpatch (from visdom->torchnet)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from visdom->torchnet) (1.7.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from visdom->torchnet) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchnet) (2.1.5)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch->visdom->torchnet)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->visdom->torchnet) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->visdom->torchnet) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->visdom->torchnet) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->visdom->torchnet) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchnet) (1.3.0)\n",
            "Building wheels for collected packages: torchnet, visdom\n",
            "  Building wheel for torchnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchnet: filename=torchnet-0.0.4-py3-none-any.whl size=29727 sha256=5746cb6c92e5a1e6dd9b742d527bfa587071e8dd24d8a99bb79e14f115fbf5bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/ae/94/9f5edd6871983f30967ad11d60ef434c3d1b007654de4c8065\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.2.4-py3-none-any.whl size=1408195 sha256=7b62a984af33ae6f05f6c780ef84f1eb173b15f55bd214760d69754b67533fbf\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/29/49/5bed207bac4578e4d2c0c5fc0226bfd33a7e2953ea56356855\n",
            "Successfully built torchnet visdom\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jsonpointer, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jsonpatch, visdom, nvidia-cusolver-cu12, torchnet\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-2.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 torchnet-0.0.4 visdom-0.2.4\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.2.0\n",
            "Collecting dotmap\n",
            "  Downloading dotmap-1.3.30-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: dotmap\n",
            "Successfully installed dotmap-1.3.30\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.99)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting decord\n",
            "  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from decord) (1.25.2)\n",
            "Installing collected packages: decord\n",
            "Successfully installed decord-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch==1.13.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHdcMEkH_Jwb",
        "outputId": "fe92505d-7056-4d11-f415-7408513ee136"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.13.0\n",
            "  Downloading torch-1.13.0-cp310-cp310-manylinux1_x86_64.whl (890.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.1/890.1 MB\u001b[0m \u001b[31m915.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0) (4.10.0)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.0)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.0)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.0)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0) (0.43.0)\n",
            "Installing collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.1+cu121\n",
            "    Uninstalling torch-2.2.1+cu121:\n",
            "      Successfully uninstalled torch-2.2.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 1.13.0 which is incompatible.\n",
            "torchdata 0.7.1 requires torch>=2, but you have torch 1.13.0 which is incompatible.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 1.13.0 which is incompatible.\n",
            "torchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 1.13.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision==0.14.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zilRg9a_R7f",
        "outputId": "ee684104-8b30-4ce2-cd00-344472c5d752"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (1.13.0)\n",
            "Collecting torchvision==0.14.0\n",
            "  Downloading torchvision-0.14.0-cp310-cp310-manylinux1_x86_64.whl (24.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.3/24.3 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.0) (4.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.0) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.0) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.0) (9.4.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.43.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.0) (2024.2.2)\n",
            "Installing collected packages: torchvision\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.17.1+cu121\n",
            "    Uninstalling torchvision-0.17.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.17.1+cu121\n",
            "Successfully installed torchvision-0.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "GyKqeFRQ7XeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/BIKE\n",
        "\n",
        "config = 'configs/k400/k400_train_rgb_vitb-32-f8.yaml'\n",
        "now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "!python -m torch.distributed.launch --nproc_per_node=1 \\\n",
        "         train.py  --config ${config} --log_time $now"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLDYA7Fy7o5g",
        "outputId": "175fa183-983c-45c7-f17c-e19c924e987b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/BIKE\n",
            "/usr/local/lib/python3.10/dist-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated\n",
            "and will be removed in future. Use torchrun.\n",
            "Note that --use_env is set by default in torchrun.\n",
            "If your script expects `--local_rank` argument to be set, please\n",
            "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
            "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
            "further instructions\n",
            "\n",
            "  warnings.warn(\n",
            "[INFO] turn on distributed train\n",
            "\u001b[32m[03/16 18:32:06 BIKE]: \u001b[0m------------------------------------\n",
            "\u001b[32m[03/16 18:32:06 BIKE]: \u001b[0mEnvironment Versions:\n",
            "\u001b[32m[03/16 18:32:06 BIKE]: \u001b[0m- Python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "\u001b[32m[03/16 18:32:06 BIKE]: \u001b[0m- PyTorch: 1.13.0+cu117\n",
            "\u001b[32m[03/16 18:32:06 BIKE]: \u001b[0m- TorchVison: 0.14.0+cu117\n",
            "\u001b[32m[03/16 18:32:06 BIKE]: \u001b[0m------------------------------------\n",
            "\u001b[32m[03/16 18:32:06 BIKE]: \u001b[0m{   'data': {   'batch_size': 32,\n",
            "                'dataset': 'k400',\n",
            "                'image_tmpl': 'img_{:05d}.jpg',\n",
            "                'input_size': 224,\n",
            "                'label_list': '/content/labels.csv',\n",
            "                'modality': 'video',\n",
            "                'num_classes': 20,\n",
            "                'num_segments': 8,\n",
            "                'output_path': 'exps',\n",
            "                'random_shift': True,\n",
            "                'seg_length': 20,\n",
            "                'train_list': '/content/bodyl_train.txt',\n",
            "                'train_root': '/content/data',\n",
            "                'val_list': '/content/bodyl_val.txt',\n",
            "                'val_root': '/content/data_BIKE_val',\n",
            "                'workers': 2},\n",
            "    'logging': {'eval_freq': 1, 'print_freq': 10},\n",
            "    'network': {   'arch': 'ViT-B/32',\n",
            "                   'drop': 0,\n",
            "                   'drop_out': 0.0,\n",
            "                   'emb_dropout': 0.0,\n",
            "                   'fix_text': True,\n",
            "                   'fix_video': False,\n",
            "                   'init': True,\n",
            "                   'interaction': 'VCS',\n",
            "                   'joint_st': False,\n",
            "                   'sim_header': 'Transf',\n",
            "                   'tm': False},\n",
            "    'pretrain': None,\n",
            "    'resume': None,\n",
            "    'seed': 1024,\n",
            "    'solver': {   'clip_ratio': 0.1,\n",
            "                  'epoch_offset': 0,\n",
            "                  'epochs': 30,\n",
            "                  'evaluate': False,\n",
            "                  'grad_accumulation_steps': 1,\n",
            "                  'loss_type': 'NCE',\n",
            "                  'lr': 5e-05,\n",
            "                  'lr_warmup_step': 5,\n",
            "                  'optim': 'adamw',\n",
            "                  'start_epoch': 0,\n",
            "                  'type': 'cosine',\n",
            "                  'weight_decay': 0.2}}\n",
            "\u001b[32m[03/16 18:32:06 BIKE]: \u001b[0m------------------------------------\n",
            "\u001b[32m[03/16 18:32:06 BIKE]: \u001b[0mstoring name: exps/k400/ViT-B/32/20240316_183157\n",
            "dropout used:[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "dropout used:[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "loading clip pretrained model!\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "\u001b[32m[03/16 18:32:11 BIKE]: \u001b[0mtrain transforms: [Compose(\n",
            "    <datasets.transforms.GroupScale object at 0x79991596bf10>\n",
            "    Compose(\n",
            "    <datasets.transforms.GroupRandomSizedCrop object at 0x79991596bcd0>\n",
            "    <datasets.transforms.GroupRandomHorizontalFlip object at 0x79991596baf0>\n",
            ")\n",
            "    <datasets.transforms.GroupRandomGrayscale object at 0x79991596bd30>\n",
            "), Compose(\n",
            "    <datasets.transforms.Stack object at 0x79991596bfd0>\n",
            "    <datasets.transforms.ToTorchFormatTensor object at 0x79991596bfa0>\n",
            "    <datasets.transforms.GroupNormalize object at 0x79991bdd3340>\n",
            ")]\n",
            "\u001b[32m[03/16 18:32:11 BIKE]: \u001b[0mval transforms: [Compose(\n",
            "    <datasets.transforms.GroupScale object at 0x79991596bc10>\n",
            "    <datasets.transforms.GroupCenterCrop object at 0x79991596b310>\n",
            "), Compose(\n",
            "    <datasets.transforms.Stack object at 0x79991596bb20>\n",
            "    <datasets.transforms.ToTorchFormatTensor object at 0x79991596b700>\n",
            "    <datasets.transforms.GroupNormalize object at 0x79991596bca0>\n",
            ")]\n",
            "layer=6\n",
            "video number:2954\n",
            "video number:1981\n",
            "=========using NCE Loss==========\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "/content/drive/MyDrive/BIKE/train.py:373: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  ground_truth = torch.tensor(gen_label(list_id),dtype=image_embedding.dtype,device=device)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2916: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  warnings.warn(\n",
            "\u001b[32m[03/16 18:32:18 BIKE]: \u001b[0mEpoch: [0][0/92], lr: 0.00e+00, eta: 4:55:11\tTime 6.415 (6.415)\tData 1.684 (1.684)\tLoss 3.7742 (3.7742)\n",
            "\u001b[32m[03/16 18:32:29 BIKE]: \u001b[0mEpoch: [0][10/92], lr: 9.78e-07, eta: 1:09:52\tTime 0.942 (1.524)\tData 0.074 (0.242)\tLoss 3.3233 (3.3143)\n",
            "\u001b[32m[03/16 18:32:40 BIKE]: \u001b[0mEpoch: [0][20/92], lr: 2.07e-06, eta: 1:02:04\tTime 0.980 (1.359)\tData 0.072 (0.236)\tLoss 3.0352 (3.2519)\n",
            "\u001b[32m[03/16 18:32:53 BIKE]: \u001b[0mEpoch: [0][30/92], lr: 3.15e-06, eta: 0:59:51\tTime 1.002 (1.315)\tData 0.057 (0.248)\tLoss 2.6586 (3.1176)\n",
            "\u001b[32m[03/16 18:33:05 BIKE]: \u001b[0mEpoch: [0][40/92], lr: 4.24e-06, eta: 0:58:39\tTime 1.161 (1.293)\tData 0.143 (0.249)\tLoss 2.5466 (3.0111)\n",
            "\u001b[32m[03/16 18:33:16 BIKE]: \u001b[0mEpoch: [0][50/92], lr: 5.33e-06, eta: 0:57:00\tTime 1.153 (1.262)\tData 0.063 (0.230)\tLoss 2.7561 (2.9309)\n",
            "\u001b[32m[03/16 18:33:28 BIKE]: \u001b[0mEpoch: [0][60/92], lr: 6.41e-06, eta: 0:56:11\tTime 0.974 (1.248)\tData 0.048 (0.230)\tLoss 2.4226 (2.8492)\n",
            "\u001b[32m[03/16 18:33:40 BIKE]: \u001b[0mEpoch: [0][70/92], lr: 7.50e-06, eta: 0:55:47\tTime 1.007 (1.244)\tData 0.064 (0.234)\tLoss 2.5905 (2.7755)\n",
            "\u001b[32m[03/16 18:33:52 BIKE]: \u001b[0mEpoch: [0][80/92], lr: 8.59e-06, eta: 0:55:20\tTime 0.981 (1.238)\tData 0.063 (0.233)\tLoss 2.2084 (2.7052)\n",
            "\u001b[32m[03/16 18:34:04 BIKE]: \u001b[0mEpoch: [0][90/92], lr: 9.67e-06, eta: 0:54:49\tTime 0.942 (1.232)\tData 0.052 (0.231)\tLoss 1.8218 (2.6413)\n",
            "\u001b[32m[03/16 18:34:07 BIKE]: \u001b[0mTest: [0/62]\tPrec@1 46.875 (46.875)\tPrec@5 93.750 (93.750)\n",
            "\u001b[32m[03/16 18:34:16 BIKE]: \u001b[0mTest: [10/62]\tPrec@1 59.375 (53.409)\tPrec@5 100.000 (96.591)\n",
            "\u001b[32m[03/16 18:34:25 BIKE]: \u001b[0mTest: [20/62]\tPrec@1 21.875 (51.042)\tPrec@5 62.500 (93.452)\n",
            "\u001b[32m[03/16 18:34:33 BIKE]: \u001b[0mTest: [30/62]\tPrec@1 18.750 (42.440)\tPrec@5 96.875 (88.810)\n",
            "\u001b[32m[03/16 18:34:43 BIKE]: \u001b[0mTest: [40/62]\tPrec@1 0.000 (34.604)\tPrec@5 28.125 (79.573)\n",
            "\u001b[32m[03/16 18:34:51 BIKE]: \u001b[0mTest: [50/62]\tPrec@1 21.875 (30.576)\tPrec@5 56.250 (72.978)\n",
            "\u001b[32m[03/16 18:35:00 BIKE]: \u001b[0mTest: [60/62]\tPrec@1 71.875 (32.684)\tPrec@5 78.125 (73.207)\n",
            "\u001b[32m[03/16 18:35:01 BIKE]: \u001b[0mTesting Results: Prec@1 33.115 Prec@5 73.397\n",
            "\u001b[32m[03/16 18:35:01 BIKE]: \u001b[0mTesting: 33.11458859739732/33.11458859739732\n",
            "\u001b[32m[03/16 18:35:01 BIKE]: \u001b[0mSaving:\n",
            "\u001b[32m[03/16 18:35:23 BIKE]: \u001b[0mEpoch: [1][0/92], lr: 1.00e-05, eta: 2:26:19\tTime 3.289 (3.289)\tData 2.114 (2.114)\tLoss 2.0667 (2.0667)\n",
            "\u001b[32m[03/16 18:35:38 BIKE]: \u001b[0mEpoch: [1][10/92], lr: 1.10e-05, eta: 1:12:43\tTime 1.724 (1.641)\tData 0.750 (0.596)\tLoss 2.0985 (2.0106)\n",
            "\u001b[32m[03/16 18:35:53 BIKE]: \u001b[0mEpoch: [1][20/92], lr: 1.21e-05, eta: 1:09:33\tTime 1.690 (1.576)\tData 0.499 (0.525)\tLoss 2.2806 (1.9430)\n",
            "\u001b[32m[03/16 18:36:08 BIKE]: \u001b[0mEpoch: [1][30/92], lr: 1.32e-05, eta: 1:08:01\tTime 1.224 (1.547)\tData 0.241 (0.484)\tLoss 1.6548 (1.8495)\n",
            "\u001b[32m[03/16 18:36:24 BIKE]: \u001b[0mEpoch: [1][40/92], lr: 1.42e-05, eta: 1:08:04\tTime 1.155 (1.553)\tData 0.227 (0.469)\tLoss 1.3997 (1.7455)\n",
            "\u001b[32m[03/16 18:36:36 BIKE]: \u001b[0mEpoch: [1][50/92], lr: 1.53e-05, eta: 1:04:47\tTime 1.270 (1.484)\tData 0.305 (0.424)\tLoss 1.1959 (1.6555)\n",
            "\u001b[32m[03/16 18:36:48 BIKE]: \u001b[0mEpoch: [1][60/92], lr: 1.64e-05, eta: 1:02:34\tTime 1.247 (1.439)\tData 0.322 (0.391)\tLoss 0.8490 (1.5674)\n",
            "\u001b[32m[03/16 18:37:00 BIKE]: \u001b[0mEpoch: [1][70/92], lr: 1.75e-05, eta: 1:00:54\tTime 1.570 (1.406)\tData 0.654 (0.371)\tLoss 0.7802 (1.4974)\n",
            "\u001b[32m[03/16 18:37:11 BIKE]: \u001b[0mEpoch: [1][80/92], lr: 1.86e-05, eta: 0:59:14\tTime 1.546 (1.373)\tData 0.454 (0.344)\tLoss 1.0607 (1.4260)\n",
            "\u001b[32m[03/16 18:37:22 BIKE]: \u001b[0mEpoch: [1][90/92], lr: 1.97e-05, eta: 0:57:46\tTime 1.001 (1.344)\tData 0.107 (0.322)\tLoss 0.8774 (1.3638)\n",
            "\u001b[32m[03/16 18:37:27 BIKE]: \u001b[0mTest: [0/62]\tPrec@1 71.875 (71.875)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 18:37:35 BIKE]: \u001b[0mTest: [10/62]\tPrec@1 100.000 (89.205)\tPrec@5 100.000 (98.864)\n",
            "\u001b[32m[03/16 18:37:45 BIKE]: \u001b[0mTest: [20/62]\tPrec@1 62.500 (88.393)\tPrec@5 100.000 (99.256)\n",
            "\u001b[32m[03/16 18:37:53 BIKE]: \u001b[0mTest: [30/62]\tPrec@1 100.000 (86.996)\tPrec@5 100.000 (99.093)\n",
            "\u001b[32m[03/16 18:38:02 BIKE]: \u001b[0mTest: [40/62]\tPrec@1 25.000 (80.335)\tPrec@5 100.000 (98.476)\n",
            "\u001b[32m[03/16 18:38:11 BIKE]: \u001b[0mTest: [50/62]\tPrec@1 78.125 (75.797)\tPrec@5 90.625 (97.855)\n",
            "\u001b[32m[03/16 18:38:19 BIKE]: \u001b[0mTest: [60/62]\tPrec@1 87.500 (77.510)\tPrec@5 100.000 (97.900)\n",
            "\u001b[32m[03/16 18:38:20 BIKE]: \u001b[0mTesting Results: Prec@1 77.839 Prec@5 97.930\n",
            "\u001b[32m[03/16 18:38:20 BIKE]: \u001b[0mTesting: 77.83947501261989/77.83947501261989\n",
            "\u001b[32m[03/16 18:38:20 BIKE]: \u001b[0mSaving:\n",
            "\u001b[32m[03/16 18:38:51 BIKE]: \u001b[0mEpoch: [2][0/92], lr: 2.00e-05, eta: 2:15:00\tTime 3.143 (3.143)\tData 2.068 (2.068)\tLoss 0.6796 (0.6796)\n",
            "\u001b[32m[03/16 18:39:04 BIKE]: \u001b[0mEpoch: [2][10/92], lr: 2.10e-05, eta: 1:03:53\tTime 1.486 (1.493)\tData 0.549 (0.504)\tLoss 0.7714 (0.8432)\n",
            "\u001b[32m[03/16 18:39:18 BIKE]: \u001b[0mEpoch: [2][20/92], lr: 2.21e-05, eta: 1:00:32\tTime 1.477 (1.421)\tData 0.415 (0.418)\tLoss 0.7456 (0.8266)\n",
            "\u001b[32m[03/16 18:39:31 BIKE]: \u001b[0mEpoch: [2][30/92], lr: 2.32e-05, eta: 0:59:08\tTime 1.358 (1.393)\tData 0.433 (0.382)\tLoss 0.9089 (0.8043)\n",
            "\u001b[32m[03/16 18:39:45 BIKE]: \u001b[0mEpoch: [2][40/92], lr: 2.42e-05, eta: 0:58:55\tTime 1.297 (1.394)\tData 0.328 (0.369)\tLoss 0.8860 (0.7964)\n",
            "\u001b[32m[03/16 18:39:59 BIKE]: \u001b[0mEpoch: [2][50/92], lr: 2.53e-05, eta: 0:58:47\tTime 1.424 (1.396)\tData 0.377 (0.367)\tLoss 0.9307 (0.7860)\n",
            "\u001b[32m[03/16 18:40:12 BIKE]: \u001b[0mEpoch: [2][60/92], lr: 2.64e-05, eta: 0:57:31\tTime 1.339 (1.371)\tData 0.407 (0.352)\tLoss 0.6410 (0.7801)\n",
            "\u001b[32m[03/16 18:40:23 BIKE]: \u001b[0mEpoch: [2][70/92], lr: 2.75e-05, eta: 0:56:01\tTime 1.932 (1.341)\tData 0.849 (0.331)\tLoss 0.7178 (0.7609)\n",
            "\u001b[32m[03/16 18:40:34 BIKE]: \u001b[0mEpoch: [2][80/92], lr: 2.86e-05, eta: 0:54:36\tTime 1.229 (1.312)\tData 0.197 (0.310)\tLoss 0.7033 (0.7664)\n",
            "\u001b[32m[03/16 18:40:46 BIKE]: \u001b[0mEpoch: [2][90/92], lr: 2.97e-05, eta: 0:53:47\tTime 0.929 (1.298)\tData 0.034 (0.301)\tLoss 0.6834 (0.7593)\n",
            "\u001b[32m[03/16 18:40:50 BIKE]: \u001b[0mTest: [0/62]\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 18:40:59 BIKE]: \u001b[0mTest: [10/62]\tPrec@1 100.000 (96.307)\tPrec@5 100.000 (98.864)\n",
            "\u001b[32m[03/16 18:41:08 BIKE]: \u001b[0mTest: [20/62]\tPrec@1 87.500 (96.131)\tPrec@5 100.000 (99.107)\n",
            "\u001b[32m[03/16 18:41:17 BIKE]: \u001b[0mTest: [30/62]\tPrec@1 100.000 (96.169)\tPrec@5 100.000 (99.395)\n",
            "\u001b[32m[03/16 18:41:26 BIKE]: \u001b[0mTest: [40/62]\tPrec@1 43.750 (89.939)\tPrec@5 96.875 (98.933)\n",
            "\u001b[32m[03/16 18:41:36 BIKE]: \u001b[0mTest: [50/62]\tPrec@1 81.250 (87.561)\tPrec@5 96.875 (98.529)\n",
            "\u001b[32m[03/16 18:41:44 BIKE]: \u001b[0mTest: [60/62]\tPrec@1 96.875 (86.936)\tPrec@5 100.000 (98.617)\n",
            "\u001b[32m[03/16 18:41:45 BIKE]: \u001b[0mTesting Results: Prec@1 87.128 Prec@5 98.637\n",
            "\u001b[32m[03/16 18:41:45 BIKE]: \u001b[0mTesting: 87.12771327612317/87.12771327612317\n",
            "\u001b[32m[03/16 18:41:45 BIKE]: \u001b[0mSaving:\n",
            "\u001b[32m[03/16 18:42:08 BIKE]: \u001b[0mEpoch: [3][0/92], lr: 3.00e-05, eta: 2:09:55\tTime 3.137 (3.137)\tData 2.019 (2.019)\tLoss 0.4023 (0.4023)\n",
            "\u001b[32m[03/16 18:42:21 BIKE]: \u001b[0mEpoch: [3][10/92], lr: 3.10e-05, eta: 1:01:16\tTime 1.502 (1.485)\tData 0.586 (0.494)\tLoss 0.5972 (0.5793)\n",
            "\u001b[32m[03/16 18:42:34 BIKE]: \u001b[0mEpoch: [3][20/92], lr: 3.21e-05, eta: 0:58:22\tTime 1.227 (1.421)\tData 0.220 (0.413)\tLoss 0.6907 (0.5874)\n",
            "\u001b[32m[03/16 18:42:48 BIKE]: \u001b[0mEpoch: [3][30/92], lr: 3.32e-05, eta: 0:56:57\tTime 1.227 (1.392)\tData 0.248 (0.368)\tLoss 0.5592 (0.5859)\n",
            "\u001b[32m[03/16 18:43:01 BIKE]: \u001b[0mEpoch: [3][40/92], lr: 3.42e-05, eta: 0:55:57\tTime 1.328 (1.373)\tData 0.380 (0.348)\tLoss 0.2981 (0.5617)\n",
            "\u001b[32m[03/16 18:43:14 BIKE]: \u001b[0mEpoch: [3][50/92], lr: 3.53e-05, eta: 0:55:22\tTime 1.307 (1.364)\tData 0.386 (0.348)\tLoss 0.8456 (0.5609)\n",
            "\u001b[32m[03/16 18:43:27 BIKE]: \u001b[0mEpoch: [3][60/92], lr: 3.64e-05, eta: 0:54:54\tTime 1.438 (1.359)\tData 0.463 (0.344)\tLoss 0.5537 (0.5541)\n",
            "\u001b[32m[03/16 18:43:39 BIKE]: \u001b[0mEpoch: [3][70/92], lr: 3.75e-05, eta: 0:53:44\tTime 1.228 (1.335)\tData 0.288 (0.327)\tLoss 0.4219 (0.5492)\n",
            "\u001b[32m[03/16 18:43:51 BIKE]: \u001b[0mEpoch: [3][80/92], lr: 3.86e-05, eta: 0:52:49\tTime 1.340 (1.318)\tData 0.422 (0.314)\tLoss 0.4459 (0.5329)\n",
            "\u001b[32m[03/16 18:44:02 BIKE]: \u001b[0mEpoch: [3][90/92], lr: 3.97e-05, eta: 0:51:46\tTime 1.297 (1.297)\tData 0.378 (0.297)\tLoss 0.2439 (0.5263)\n",
            "\u001b[32m[03/16 18:44:06 BIKE]: \u001b[0mTest: [0/62]\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 18:44:14 BIKE]: \u001b[0mTest: [10/62]\tPrec@1 100.000 (94.602)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 18:44:23 BIKE]: \u001b[0mTest: [20/62]\tPrec@1 90.625 (96.280)\tPrec@5 96.875 (99.702)\n",
            "\u001b[32m[03/16 18:44:33 BIKE]: \u001b[0mTest: [30/62]\tPrec@1 100.000 (97.077)\tPrec@5 100.000 (99.798)\n",
            "\u001b[32m[03/16 18:44:41 BIKE]: \u001b[0mTest: [40/62]\tPrec@1 28.125 (93.293)\tPrec@5 96.875 (99.695)\n",
            "\u001b[32m[03/16 18:44:51 BIKE]: \u001b[0mTest: [50/62]\tPrec@1 81.250 (88.235)\tPrec@5 90.625 (98.958)\n",
            "\u001b[32m[03/16 18:44:59 BIKE]: \u001b[0mTest: [60/62]\tPrec@1 100.000 (89.191)\tPrec@5 100.000 (99.078)\n",
            "\u001b[32m[03/16 18:45:00 BIKE]: \u001b[0mTesting Results: Prec@1 89.349 Prec@5 99.091\n",
            "\u001b[32m[03/16 18:45:00 BIKE]: \u001b[0mTesting: 89.34881373043918/89.34881373043918\n",
            "\u001b[32m[03/16 18:45:00 BIKE]: \u001b[0mSaving:\n",
            "\u001b[32m[03/16 18:45:27 BIKE]: \u001b[0mEpoch: [4][0/92], lr: 4.00e-05, eta: 3:12:52\tTime 4.836 (4.836)\tData 3.714 (3.714)\tLoss 0.7333 (0.7333)\n",
            "\u001b[32m[03/16 18:45:41 BIKE]: \u001b[0mEpoch: [4][10/92], lr: 4.10e-05, eta: 1:09:35\tTime 2.021 (1.752)\tData 0.983 (0.722)\tLoss 0.3735 (0.5565)\n",
            "\u001b[32m[03/16 18:45:55 BIKE]: \u001b[0mEpoch: [4][20/92], lr: 4.21e-05, eta: 1:02:02\tTime 1.758 (1.569)\tData 0.696 (0.538)\tLoss 0.3947 (0.5198)\n",
            "\u001b[32m[03/16 18:46:09 BIKE]: \u001b[0mEpoch: [4][30/92], lr: 4.32e-05, eta: 0:59:12\tTime 2.089 (1.503)\tData 1.036 (0.473)\tLoss 0.2835 (0.4695)\n",
            "\u001b[32m[03/16 18:46:22 BIKE]: \u001b[0mEpoch: [4][40/92], lr: 4.42e-05, eta: 0:57:28\tTime 2.344 (1.466)\tData 1.327 (0.442)\tLoss 0.4498 (0.4553)\n",
            "\u001b[32m[03/16 18:46:35 BIKE]: \u001b[0mEpoch: [4][50/92], lr: 4.53e-05, eta: 0:56:02\tTime 2.141 (1.435)\tData 1.053 (0.415)\tLoss 0.3400 (0.4431)\n",
            "\u001b[32m[03/16 18:46:46 BIKE]: \u001b[0mEpoch: [4][60/92], lr: 4.64e-05, eta: 0:53:47\tTime 1.241 (1.383)\tData 0.112 (0.373)\tLoss 0.4429 (0.4446)\n",
            "\u001b[32m[03/16 18:46:58 BIKE]: \u001b[0mEpoch: [4][70/92], lr: 4.75e-05, eta: 0:52:27\tTime 1.130 (1.355)\tData 0.226 (0.355)\tLoss 0.4849 (0.4373)\n",
            "\u001b[32m[03/16 18:47:10 BIKE]: \u001b[0mEpoch: [4][80/92], lr: 4.86e-05, eta: 0:51:30\tTime 1.238 (1.336)\tData 0.297 (0.341)\tLoss 0.2194 (0.4267)\n",
            "\u001b[32m[03/16 18:47:22 BIKE]: \u001b[0mEpoch: [4][90/92], lr: 4.97e-05, eta: 0:50:34\tTime 0.944 (1.317)\tData 0.041 (0.324)\tLoss 0.4699 (0.4227)\n",
            "\u001b[32m[03/16 18:47:25 BIKE]: \u001b[0mTest: [0/62]\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 18:47:35 BIKE]: \u001b[0mTest: [10/62]\tPrec@1 100.000 (98.864)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 18:47:43 BIKE]: \u001b[0mTest: [20/62]\tPrec@1 93.750 (98.214)\tPrec@5 100.000 (99.702)\n",
            "\u001b[32m[03/16 18:47:52 BIKE]: \u001b[0mTest: [30/62]\tPrec@1 100.000 (98.488)\tPrec@5 100.000 (99.698)\n",
            "\u001b[32m[03/16 18:48:02 BIKE]: \u001b[0mTest: [40/62]\tPrec@1 84.375 (97.332)\tPrec@5 96.875 (99.162)\n",
            "\u001b[32m[03/16 18:48:10 BIKE]: \u001b[0mTest: [50/62]\tPrec@1 87.500 (96.140)\tPrec@5 100.000 (99.081)\n",
            "\u001b[32m[03/16 18:48:19 BIKE]: \u001b[0mTest: [60/62]\tPrec@1 100.000 (95.184)\tPrec@5 100.000 (99.027)\n",
            "\u001b[32m[03/16 18:48:20 BIKE]: \u001b[0mTesting Results: Prec@1 95.255 Prec@5 99.041\n",
            "\u001b[32m[03/16 18:48:20 BIKE]: \u001b[0mTesting: 95.25492175668855/95.25492175668855\n",
            "\u001b[32m[03/16 18:48:20 BIKE]: \u001b[0mSaving:\n",
            "\u001b[32m[03/16 18:48:40 BIKE]: \u001b[0mEpoch: [5][0/92], lr: 5.00e-05, eta: 2:45:13\tTime 4.308 (4.308)\tData 3.211 (3.211)\tLoss 0.4388 (0.4388)\n",
            "\u001b[32m[03/16 18:48:53 BIKE]: \u001b[0mEpoch: [5][10/92], lr: 5.00e-05, eta: 1:02:40\tTime 1.465 (1.641)\tData 0.538 (0.615)\tLoss 0.3128 (0.3707)\n",
            "\u001b[32m[03/16 18:49:07 BIKE]: \u001b[0mEpoch: [5][20/92], lr: 5.00e-05, eta: 0:57:12\tTime 2.451 (1.505)\tData 1.401 (0.483)\tLoss 0.4270 (0.3869)\n",
            "\u001b[32m[03/16 18:49:20 BIKE]: \u001b[0mEpoch: [5][30/92], lr: 5.00e-05, eta: 0:54:30\tTime 2.075 (1.440)\tData 0.873 (0.414)\tLoss 0.1636 (0.3698)\n",
            "\u001b[32m[03/16 18:49:33 BIKE]: \u001b[0mEpoch: [5][40/92], lr: 5.00e-05, eta: 0:53:17\tTime 2.300 (1.414)\tData 1.100 (0.396)\tLoss 0.2470 (0.3677)\n",
            "\u001b[32m[03/16 18:49:46 BIKE]: \u001b[0mEpoch: [5][50/92], lr: 4.99e-05, eta: 0:52:10\tTime 2.199 (1.391)\tData 1.072 (0.380)\tLoss 0.5899 (0.3761)\n",
            "\u001b[32m[03/16 18:49:59 BIKE]: \u001b[0mEpoch: [5][60/92], lr: 4.99e-05, eta: 0:51:07\tTime 1.461 (1.369)\tData 0.392 (0.363)\tLoss 0.4567 (0.3929)\n",
            "\u001b[32m[03/16 18:50:11 BIKE]: \u001b[0mEpoch: [5][70/92], lr: 4.99e-05, eta: 0:50:01\tTime 1.132 (1.345)\tData 0.180 (0.346)\tLoss 0.5592 (0.4025)\n",
            "\u001b[32m[03/16 18:50:23 BIKE]: \u001b[0mEpoch: [5][80/92], lr: 4.99e-05, eta: 0:49:07\tTime 1.222 (1.327)\tData 0.281 (0.330)\tLoss 0.4570 (0.3963)\n",
            "\u001b[32m[03/16 18:50:35 BIKE]: \u001b[0mEpoch: [5][90/92], lr: 4.98e-05, eta: 0:48:19\tTime 1.010 (1.311)\tData 0.104 (0.315)\tLoss 0.5115 (0.3921)\n",
            "\u001b[32m[03/16 18:50:38 BIKE]: \u001b[0mTest: [0/62]\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 18:50:48 BIKE]: \u001b[0mTest: [10/62]\tPrec@1 100.000 (98.864)\tPrec@5 100.000 (98.864)\n",
            "\u001b[32m[03/16 18:50:56 BIKE]: \u001b[0mTest: [20/62]\tPrec@1 93.750 (97.173)\tPrec@5 100.000 (99.256)\n",
            "\u001b[32m[03/16 18:51:05 BIKE]: \u001b[0mTest: [30/62]\tPrec@1 100.000 (97.984)\tPrec@5 100.000 (99.496)\n",
            "\u001b[32m[03/16 18:51:15 BIKE]: \u001b[0mTest: [40/62]\tPrec@1 78.125 (96.037)\tPrec@5 96.875 (99.390)\n",
            "\u001b[32m[03/16 18:51:23 BIKE]: \u001b[0mTest: [50/62]\tPrec@1 78.125 (94.056)\tPrec@5 93.750 (98.958)\n",
            "\u001b[32m[03/16 18:51:32 BIKE]: \u001b[0mTest: [60/62]\tPrec@1 100.000 (93.852)\tPrec@5 100.000 (99.027)\n",
            "\u001b[32m[03/16 18:51:33 BIKE]: \u001b[0mTesting Results: Prec@1 93.942 Prec@5 99.041\n",
            "\u001b[32m[03/16 18:51:33 BIKE]: \u001b[0mTesting: 93.9424533064109/95.25492175668855\n",
            "\u001b[32m[03/16 18:51:33 BIKE]: \u001b[0mSaving:\n",
            "\u001b[32m[03/16 18:51:51 BIKE]: \u001b[0mEpoch: [6][0/92], lr: 4.98e-05, eta: 2:16:14\tTime 3.701 (3.701)\tData 2.447 (2.447)\tLoss 0.3929 (0.3929)\n",
            "\u001b[32m[03/16 18:52:03 BIKE]: \u001b[0mEpoch: [6][10/92], lr: 4.98e-05, eta: 0:51:58\tTime 1.339 (1.418)\tData 0.339 (0.439)\tLoss 0.2322 (0.3396)\n",
            "\u001b[32m[03/16 18:52:16 BIKE]: \u001b[0mEpoch: [6][20/92], lr: 4.97e-05, eta: 0:49:53\tTime 1.109 (1.367)\tData 0.113 (0.363)\tLoss 0.3679 (0.3406)\n",
            "\u001b[32m[03/16 18:52:29 BIKE]: \u001b[0mEpoch: [6][30/92], lr: 4.97e-05, eta: 0:48:40\tTime 1.067 (1.340)\tData 0.107 (0.324)\tLoss 0.4587 (0.3591)\n",
            "\u001b[32m[03/16 18:52:41 BIKE]: \u001b[0mEpoch: [6][40/92], lr: 4.96e-05, eta: 0:47:24\tTime 1.114 (1.312)\tData 0.142 (0.294)\tLoss 0.3068 (0.3360)\n",
            "\u001b[32m[03/16 18:52:52 BIKE]: \u001b[0mEpoch: [6][50/92], lr: 4.95e-05, eta: 0:45:58\tTime 1.086 (1.278)\tData 0.116 (0.265)\tLoss 0.4125 (0.3361)\n",
            "\u001b[32m[03/16 18:53:04 BIKE]: \u001b[0mEpoch: [6][60/92], lr: 4.95e-05, eta: 0:44:59\tTime 1.322 (1.256)\tData 0.419 (0.251)\tLoss 0.3547 (0.3318)\n",
            "\u001b[32m[03/16 18:53:15 BIKE]: \u001b[0mEpoch: [6][70/92], lr: 4.94e-05, eta: 0:44:26\tTime 1.971 (1.247)\tData 0.902 (0.244)\tLoss 0.1394 (0.3325)\n",
            "\u001b[32m[03/16 18:53:26 BIKE]: \u001b[0mEpoch: [6][80/92], lr: 4.93e-05, eta: 0:43:33\tTime 1.385 (1.228)\tData 0.306 (0.231)\tLoss 0.2634 (0.3331)\n",
            "\u001b[32m[03/16 18:53:38 BIKE]: \u001b[0mEpoch: [6][90/92], lr: 4.92e-05, eta: 0:42:58\tTime 0.935 (1.217)\tData 0.044 (0.226)\tLoss 0.2874 (0.3268)\n",
            "\u001b[32m[03/16 18:53:42 BIKE]: \u001b[0mTest: [0/62]\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 18:53:51 BIKE]: \u001b[0mTest: [10/62]\tPrec@1 100.000 (97.443)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 18:54:00 BIKE]: \u001b[0mTest: [20/62]\tPrec@1 93.750 (97.619)\tPrec@5 100.000 (99.554)\n",
            "\u001b[32m[03/16 18:54:08 BIKE]: \u001b[0mTest: [30/62]\tPrec@1 100.000 (97.278)\tPrec@5 100.000 (99.496)\n",
            "\u001b[32m[03/16 18:54:17 BIKE]: \u001b[0mTest: [40/62]\tPrec@1 96.875 (97.104)\tPrec@5 100.000 (99.619)\n",
            "\u001b[32m[03/16 18:54:26 BIKE]: \u001b[0mTest: [50/62]\tPrec@1 78.125 (96.078)\tPrec@5 100.000 (99.510)\n",
            "\u001b[32m[03/16 18:54:34 BIKE]: \u001b[0mTest: [60/62]\tPrec@1 100.000 (95.441)\tPrec@5 100.000 (99.488)\n",
            "\u001b[32m[03/16 18:54:35 BIKE]: \u001b[0mTesting Results: Prec@1 95.507 Prec@5 99.495\n",
            "\u001b[32m[03/16 18:54:35 BIKE]: \u001b[0mTesting: 95.50731953558808/95.50731953558808\n",
            "\u001b[32m[03/16 18:54:35 BIKE]: \u001b[0mSaving:\n",
            "\u001b[32m[03/16 18:54:59 BIKE]: \u001b[0mEpoch: [7][0/92], lr: 4.92e-05, eta: 1:44:17\tTime 2.956 (2.956)\tData 1.985 (1.985)\tLoss 0.3294 (0.3294)\n",
            "\u001b[32m[03/16 18:55:12 BIKE]: \u001b[0mEpoch: [7][10/92], lr: 4.91e-05, eta: 0:51:34\tTime 1.235 (1.468)\tData 0.227 (0.439)\tLoss 0.3684 (0.2556)\n",
            "\u001b[32m[03/16 18:55:25 BIKE]: \u001b[0mEpoch: [7][20/92], lr: 4.90e-05, eta: 0:48:40\tTime 1.337 (1.393)\tData 0.307 (0.366)\tLoss 0.4171 (0.2726)\n",
            "\u001b[32m[03/16 18:55:38 BIKE]: \u001b[0mEpoch: [7][30/92], lr: 4.89e-05, eta: 0:46:59\tTime 1.262 (1.351)\tData 0.315 (0.316)\tLoss 0.1181 (0.2634)\n",
            "\u001b[32m[03/16 18:55:52 BIKE]: \u001b[0mEpoch: [7][40/92], lr: 4.88e-05, eta: 0:47:10\tTime 1.597 (1.363)\tData 0.477 (0.324)\tLoss 0.2878 (0.2620)\n",
            "\u001b[32m[03/16 18:56:05 BIKE]: \u001b[0mEpoch: [7][50/92], lr: 4.87e-05, eta: 0:46:39\tTime 1.308 (1.354)\tData 0.348 (0.317)\tLoss 0.1030 (0.2756)\n",
            "\u001b[32m[03/16 18:56:18 BIKE]: \u001b[0mEpoch: [7][60/92], lr: 4.86e-05, eta: 0:46:10\tTime 1.117 (1.347)\tData 0.174 (0.310)\tLoss 0.2008 (0.2691)\n",
            "\u001b[32m[03/16 18:56:30 BIKE]: \u001b[0mEpoch: [7][70/92], lr: 4.85e-05, eta: 0:45:15\tTime 1.140 (1.327)\tData 0.197 (0.297)\tLoss 0.3844 (0.2698)\n",
            "\u001b[32m[03/16 18:56:41 BIKE]: \u001b[0mEpoch: [7][80/92], lr: 4.84e-05, eta: 0:44:20\tTime 1.656 (1.306)\tData 0.601 (0.282)\tLoss 0.1738 (0.2668)\n",
            "\u001b[32m[03/16 18:56:52 BIKE]: \u001b[0mEpoch: [7][90/92], lr: 4.83e-05, eta: 0:43:12\tTime 0.928 (1.279)\tData 0.034 (0.263)\tLoss 0.1633 (0.2666)\n",
            "\u001b[32m[03/16 18:56:56 BIKE]: \u001b[0mTest: [0/62]\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 18:57:04 BIKE]: \u001b[0mTest: [10/62]\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 18:57:13 BIKE]: \u001b[0mTest: [20/62]\tPrec@1 96.875 (98.958)\tPrec@5 100.000 (99.702)\n",
            "\u001b[32m[03/16 18:57:22 BIKE]: \u001b[0mTest: [30/62]\tPrec@1 100.000 (99.093)\tPrec@5 100.000 (99.798)\n",
            "\u001b[32m[03/16 18:57:31 BIKE]: \u001b[0mTest: [40/62]\tPrec@1 96.875 (98.857)\tPrec@5 100.000 (99.848)\n",
            "\u001b[32m[03/16 18:57:40 BIKE]: \u001b[0mTest: [50/62]\tPrec@1 81.250 (96.691)\tPrec@5 100.000 (99.632)\n",
            "\u001b[32m[03/16 18:57:48 BIKE]: \u001b[0mTest: [60/62]\tPrec@1 100.000 (96.516)\tPrec@5 100.000 (99.539)\n",
            "\u001b[32m[03/16 18:57:49 BIKE]: \u001b[0mTesting Results: Prec@1 96.567 Prec@5 99.546\n",
            "\u001b[32m[03/16 18:57:49 BIKE]: \u001b[0mTesting: 96.56739020696618/96.56739020696618\n",
            "\u001b[32m[03/16 18:57:49 BIKE]: \u001b[0mSaving:\n",
            "\u001b[32m[03/16 18:58:19 BIKE]: \u001b[0mEpoch: [8][0/92], lr: 4.82e-05, eta: 1:48:03\tTime 3.202 (3.202)\tData 2.029 (2.029)\tLoss 0.1789 (0.1789)\n",
            "\u001b[32m[03/16 18:58:33 BIKE]: \u001b[0mEpoch: [8][10/92], lr: 4.81e-05, eta: 0:51:53\tTime 1.414 (1.545)\tData 0.471 (0.441)\tLoss 0.2548 (0.2306)\n",
            "\u001b[32m[03/16 18:58:46 BIKE]: \u001b[0mEpoch: [8][20/92], lr: 4.80e-05, eta: 0:48:11\tTime 1.291 (1.442)\tData 0.271 (0.364)\tLoss 0.3038 (0.2225)\n",
            "\u001b[32m[03/16 18:59:00 BIKE]: \u001b[0mEpoch: [8][30/92], lr: 4.79e-05, eta: 0:46:46\tTime 1.257 (1.407)\tData 0.288 (0.337)\tLoss 0.2543 (0.2236)\n",
            "\u001b[32m[03/16 18:59:14 BIKE]: \u001b[0mEpoch: [8][40/92], lr: 4.77e-05, eta: 0:46:52\tTime 2.349 (1.417)\tData 1.334 (0.343)\tLoss 0.2641 (0.2200)\n",
            "\u001b[32m[03/16 18:59:28 BIKE]: \u001b[0mEpoch: [8][50/92], lr: 4.76e-05, eta: 0:46:11\tTime 1.385 (1.403)\tData 0.398 (0.335)\tLoss 0.3107 (0.2269)\n",
            "\u001b[32m[03/16 18:59:39 BIKE]: \u001b[0mEpoch: [8][60/92], lr: 4.74e-05, eta: 0:44:42\tTime 1.711 (1.365)\tData 0.745 (0.314)\tLoss 0.2431 (0.2371)\n",
            "\u001b[32m[03/16 18:59:50 BIKE]: \u001b[0mEpoch: [8][70/92], lr: 4.73e-05, eta: 0:43:12\tTime 1.398 (1.326)\tData 0.265 (0.286)\tLoss 0.2598 (0.2350)\n",
            "\u001b[32m[03/16 19:00:02 BIKE]: \u001b[0mEpoch: [8][80/92], lr: 4.71e-05, eta: 0:42:22\tTime 1.106 (1.307)\tData 0.171 (0.275)\tLoss 0.2771 (0.2417)\n",
            "\u001b[32m[03/16 19:00:14 BIKE]: \u001b[0mEpoch: [8][90/92], lr: 4.70e-05, eta: 0:41:48\tTime 0.925 (1.296)\tData 0.035 (0.268)\tLoss 0.2667 (0.2393)\n",
            "\u001b[32m[03/16 19:00:18 BIKE]: \u001b[0mTest: [0/62]\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 19:00:27 BIKE]: \u001b[0mTest: [10/62]\tPrec@1 100.000 (98.864)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 19:00:36 BIKE]: \u001b[0mTest: [20/62]\tPrec@1 93.750 (98.661)\tPrec@5 100.000 (99.851)\n",
            "\u001b[32m[03/16 19:00:45 BIKE]: \u001b[0mTest: [30/62]\tPrec@1 100.000 (98.790)\tPrec@5 100.000 (99.899)\n",
            "\u001b[32m[03/16 19:00:54 BIKE]: \u001b[0mTest: [40/62]\tPrec@1 96.875 (98.476)\tPrec@5 100.000 (99.924)\n",
            "\u001b[32m[03/16 19:01:02 BIKE]: \u001b[0mTest: [50/62]\tPrec@1 84.375 (95.833)\tPrec@5 100.000 (99.694)\n",
            "\u001b[32m[03/16 19:01:11 BIKE]: \u001b[0mTest: [60/62]\tPrec@1 100.000 (96.004)\tPrec@5 100.000 (99.590)\n",
            "\u001b[32m[03/16 19:01:11 BIKE]: \u001b[0mTesting Results: Prec@1 96.063 Prec@5 99.596\n",
            "\u001b[32m[03/16 19:01:12 BIKE]: \u001b[0mTesting: 96.06259464916708/96.56739020696618\n",
            "\u001b[32m[03/16 19:01:12 BIKE]: \u001b[0mSaving:\n",
            "\u001b[32m[03/16 19:01:23 BIKE]: \u001b[0mEpoch: [9][0/92], lr: 4.69e-05, eta: 1:32:02\tTime 2.857 (2.857)\tData 1.772 (1.772)\tLoss 0.2539 (0.2539)\n",
            "\u001b[32m[03/16 19:01:35 BIKE]: \u001b[0mEpoch: [9][10/92], lr: 4.68e-05, eta: 0:44:37\tTime 1.487 (1.392)\tData 0.500 (0.383)\tLoss 0.3939 (0.2171)\n",
            "\u001b[32m[03/16 19:01:48 BIKE]: \u001b[0mEpoch: [9][20/92], lr: 4.66e-05, eta: 0:42:43\tTime 1.133 (1.340)\tData 0.148 (0.328)\tLoss 0.3202 (0.2825)\n",
            "\u001b[32m[03/16 19:02:01 BIKE]: \u001b[0mEpoch: [9][30/92], lr: 4.64e-05, eta: 0:42:09\tTime 1.348 (1.329)\tData 0.369 (0.306)\tLoss 0.2122 (0.2825)\n",
            "\u001b[32m[03/16 19:02:13 BIKE]: \u001b[0mEpoch: [9][40/92], lr: 4.62e-05, eta: 0:41:22\tTime 1.219 (1.311)\tData 0.289 (0.288)\tLoss 0.1029 (0.2788)\n",
            "\u001b[32m[03/16 19:02:25 BIKE]: \u001b[0mEpoch: [9][50/92], lr: 4.61e-05, eta: 0:40:19\tTime 1.355 (1.285)\tData 0.450 (0.269)\tLoss 0.4425 (0.2792)\n",
            "\u001b[32m[03/16 19:02:37 BIKE]: \u001b[0mEpoch: [9][60/92], lr: 4.59e-05, eta: 0:39:22\tTime 1.559 (1.261)\tData 0.424 (0.252)\tLoss 0.2764 (0.2687)\n",
            "\u001b[32m[03/16 19:02:47 BIKE]: \u001b[0mEpoch: [9][70/92], lr: 4.57e-05, eta: 0:38:23\tTime 1.082 (1.237)\tData 0.188 (0.236)\tLoss 0.1157 (0.2667)\n",
            "\u001b[32m[03/16 19:02:59 BIKE]: \u001b[0mEpoch: [9][80/92], lr: 4.55e-05, eta: 0:37:59\tTime 1.124 (1.230)\tData 0.201 (0.231)\tLoss 0.2007 (0.2615)\n",
            "\u001b[32m[03/16 19:03:11 BIKE]: \u001b[0mEpoch: [9][90/92], lr: 4.53e-05, eta: 0:37:30\tTime 0.928 (1.221)\tData 0.036 (0.224)\tLoss 0.0912 (0.2625)\n",
            "\u001b[32m[03/16 19:03:14 BIKE]: \u001b[0mTest: [0/62]\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 19:03:24 BIKE]: \u001b[0mTest: [10/62]\tPrec@1 100.000 (98.864)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 19:03:33 BIKE]: \u001b[0mTest: [20/62]\tPrec@1 93.750 (98.512)\tPrec@5 100.000 (99.851)\n",
            "\u001b[32m[03/16 19:03:41 BIKE]: \u001b[0mTest: [30/62]\tPrec@1 100.000 (98.690)\tPrec@5 100.000 (99.899)\n",
            "\u001b[32m[03/16 19:03:50 BIKE]: \u001b[0mTest: [40/62]\tPrec@1 90.625 (97.942)\tPrec@5 96.875 (99.771)\n",
            "\u001b[32m[03/16 19:03:58 BIKE]: \u001b[0mTest: [50/62]\tPrec@1 84.375 (97.304)\tPrec@5 100.000 (99.816)\n",
            "\u001b[32m[03/16 19:04:07 BIKE]: \u001b[0mTest: [60/62]\tPrec@1 100.000 (96.670)\tPrec@5 100.000 (99.693)\n",
            "\u001b[32m[03/16 19:04:08 BIKE]: \u001b[0mTesting Results: Prec@1 96.719 Prec@5 99.697\n",
            "\u001b[32m[03/16 19:04:08 BIKE]: \u001b[0mTesting: 96.7188288743059/96.7188288743059\n",
            "\u001b[32m[03/16 19:04:08 BIKE]: \u001b[0mSaving:\n",
            "\u001b[32m[03/16 19:04:37 BIKE]: \u001b[0mEpoch: [10][0/92], lr: 4.52e-05, eta: 1:43:03\tTime 3.359 (3.359)\tData 2.203 (2.203)\tLoss 0.1143 (0.1143)\n",
            "\u001b[32m[03/16 19:04:51 BIKE]: \u001b[0mEpoch: [10][10/92], lr: 4.50e-05, eta: 0:47:49\tTime 2.040 (1.567)\tData 0.748 (0.530)\tLoss 0.1360 (0.2043)\n",
            "\u001b[32m[03/16 19:05:04 BIKE]: \u001b[0mEpoch: [10][20/92], lr: 4.48e-05, eta: 0:43:59\tTime 1.495 (1.449)\tData 0.281 (0.416)\tLoss 0.1220 (0.1847)\n",
            "\u001b[32m[03/16 19:05:18 BIKE]: \u001b[0mEpoch: [10][30/92], lr: 4.46e-05, eta: 0:42:54\tTime 1.526 (1.422)\tData 0.442 (0.387)\tLoss 0.0962 (0.1918)\n",
            "\u001b[32m[03/16 19:05:31 BIKE]: \u001b[0mEpoch: [10][40/92], lr: 4.44e-05, eta: 0:42:14\tTime 1.438 (1.407)\tData 0.315 (0.374)\tLoss 0.3176 (0.1828)\n",
            "\u001b[32m[03/16 19:05:44 BIKE]: \u001b[0mEpoch: [10][50/92], lr: 4.42e-05, eta: 0:41:28\tTime 1.349 (1.389)\tData 0.333 (0.364)\tLoss 0.1602 (0.1903)\n",
            "\u001b[32m[03/16 19:05:57 BIKE]: \u001b[0mEpoch: [10][60/92], lr: 4.40e-05, eta: 0:40:37\tTime 1.121 (1.368)\tData 0.212 (0.351)\tLoss 0.1853 (0.1931)\n",
            "\u001b[32m[03/16 19:06:09 BIKE]: \u001b[0mEpoch: [10][70/92], lr: 4.38e-05, eta: 0:39:36\tTime 1.173 (1.342)\tData 0.261 (0.330)\tLoss 0.1925 (0.1919)\n",
            "\u001b[32m[03/16 19:06:21 BIKE]: \u001b[0mEpoch: [10][80/92], lr: 4.35e-05, eta: 0:38:55\tTime 1.293 (1.326)\tData 0.297 (0.319)\tLoss 0.4798 (0.1948)\n",
            "\u001b[32m[03/16 19:06:32 BIKE]: \u001b[0mEpoch: [10][90/92], lr: 4.33e-05, eta: 0:38:06\tTime 0.924 (1.306)\tData 0.035 (0.303)\tLoss 0.4390 (0.1983)\n",
            "\u001b[32m[03/16 19:06:36 BIKE]: \u001b[0mTest: [0/62]\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 19:06:45 BIKE]: \u001b[0mTest: [10/62]\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 19:06:54 BIKE]: \u001b[0mTest: [20/62]\tPrec@1 96.875 (99.107)\tPrec@5 100.000 (99.702)\n",
            "\u001b[32m[03/16 19:07:03 BIKE]: \u001b[0mTest: [30/62]\tPrec@1 100.000 (99.194)\tPrec@5 100.000 (99.798)\n",
            "\u001b[32m[03/16 19:07:12 BIKE]: \u001b[0mTest: [40/62]\tPrec@1 96.875 (98.704)\tPrec@5 96.875 (99.619)\n",
            "\u001b[32m[03/16 19:07:20 BIKE]: \u001b[0mTest: [50/62]\tPrec@1 90.625 (98.100)\tPrec@5 100.000 (99.632)\n",
            "\u001b[32m[03/16 19:07:29 BIKE]: \u001b[0mTest: [60/62]\tPrec@1 100.000 (97.746)\tPrec@5 100.000 (99.539)\n",
            "\u001b[32m[03/16 19:07:30 BIKE]: \u001b[0mTesting Results: Prec@1 97.779 Prec@5 99.546\n",
            "\u001b[32m[03/16 19:07:30 BIKE]: \u001b[0mTesting: 97.778899545684/97.778899545684\n",
            "\u001b[32m[03/16 19:07:30 BIKE]: \u001b[0mSaving:\n",
            "\u001b[32m[03/16 19:07:48 BIKE]: \u001b[0mEpoch: [11][0/92], lr: 4.32e-05, eta: 1:27:45\tTime 3.010 (3.010)\tData 1.966 (1.966)\tLoss 0.0811 (0.0811)\n",
            "\u001b[32m[03/16 19:08:02 BIKE]: \u001b[0mEpoch: [11][10/92], lr: 4.30e-05, eta: 0:43:28\tTime 1.436 (1.500)\tData 0.498 (0.510)\tLoss 0.1074 (0.1746)\n",
            "\u001b[32m[03/16 19:08:16 BIKE]: \u001b[0mEpoch: [11][20/92], lr: 4.28e-05, eta: 0:41:31\tTime 1.399 (1.441)\tData 0.403 (0.440)\tLoss 0.2652 (0.1920)\n",
            "\u001b[32m[03/16 19:08:29 BIKE]: \u001b[0mEpoch: [11][30/92], lr: 4.25e-05, eta: 0:40:13\tTime 1.255 (1.404)\tData 0.318 (0.402)\tLoss 0.2774 (0.1812)\n",
            "\u001b[32m[03/16 19:08:42 BIKE]: \u001b[0mEpoch: [11][40/92], lr: 4.23e-05, eta: 0:39:36\tTime 1.509 (1.390)\tData 0.497 (0.386)\tLoss 0.2067 (0.2002)\n",
            "\u001b[32m[03/16 19:08:56 BIKE]: \u001b[0mEpoch: [11][50/92], lr: 4.20e-05, eta: 0:38:55\tTime 1.330 (1.374)\tData 0.380 (0.365)\tLoss 0.1544 (0.2003)\n",
            "\u001b[32m[03/16 19:09:09 BIKE]: \u001b[0mEpoch: [11][60/92], lr: 4.18e-05, eta: 0:38:26\tTime 1.260 (1.366)\tData 0.344 (0.355)\tLoss 0.1213 (0.1916)\n",
            "\u001b[32m[03/16 19:09:21 BIKE]: \u001b[0mEpoch: [11][70/92], lr: 4.15e-05, eta: 0:37:41\tTime 1.136 (1.347)\tData 0.179 (0.339)\tLoss 0.3908 (0.1885)\n",
            "\u001b[32m[03/16 19:09:33 BIKE]: \u001b[0mEpoch: [11][80/92], lr: 4.13e-05, eta: 0:36:51\tTime 1.156 (1.325)\tData 0.244 (0.322)\tLoss 0.3763 (0.1840)\n",
            "\u001b[32m[03/16 19:09:44 BIKE]: \u001b[0mEpoch: [11][90/92], lr: 4.10e-05, eta: 0:36:02\tTime 1.174 (1.303)\tData 0.263 (0.304)\tLoss 0.1073 (0.1851)\n",
            "\u001b[32m[03/16 19:09:47 BIKE]: \u001b[0mTest: [0/62]\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 19:09:56 BIKE]: \u001b[0mTest: [10/62]\tPrec@1 100.000 (98.864)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 19:10:05 BIKE]: \u001b[0mTest: [20/62]\tPrec@1 93.750 (98.661)\tPrec@5 100.000 (99.851)\n",
            "\u001b[32m[03/16 19:10:14 BIKE]: \u001b[0mTest: [30/62]\tPrec@1 100.000 (98.992)\tPrec@5 100.000 (99.798)\n",
            "\u001b[32m[03/16 19:10:22 BIKE]: \u001b[0mTest: [40/62]\tPrec@1 90.625 (98.399)\tPrec@5 100.000 (99.848)\n",
            "\u001b[32m[03/16 19:10:31 BIKE]: \u001b[0mTest: [50/62]\tPrec@1 90.625 (97.794)\tPrec@5 100.000 (99.877)\n",
            "\u001b[32m[03/16 19:10:40 BIKE]: \u001b[0mTest: [60/62]\tPrec@1 100.000 (97.490)\tPrec@5 100.000 (99.846)\n",
            "\u001b[32m[03/16 19:10:41 BIKE]: \u001b[0mTesting Results: Prec@1 97.527 Prec@5 99.849\n",
            "\u001b[32m[03/16 19:10:41 BIKE]: \u001b[0mTesting: 97.52650176678445/97.778899545684\n",
            "\u001b[32m[03/16 19:10:41 BIKE]: \u001b[0mSaving:\n",
            "\u001b[32m[03/16 19:10:50 BIKE]: \u001b[0mEpoch: [12][0/92], lr: 4.09e-05, eta: 1:19:33\tTime 2.881 (2.881)\tData 1.780 (1.780)\tLoss 0.1898 (0.1898)\n",
            "\u001b[32m[03/16 19:11:02 BIKE]: \u001b[0mEpoch: [12][10/92], lr: 4.07e-05, eta: 0:38:36\tTime 1.332 (1.407)\tData 0.437 (0.432)\tLoss 0.1757 (0.1608)\n",
            "\u001b[32m[03/16 19:11:15 BIKE]: \u001b[0mEpoch: [12][20/92], lr: 4.04e-05, eta: 0:36:51\tTime 1.335 (1.351)\tData 0.347 (0.361)\tLoss 0.2784 (0.1808)\n",
            "\u001b[32m[03/16 19:11:28 BIKE]: \u001b[0mEpoch: [12][30/92], lr: 4.02e-05, eta: 0:36:09\tTime 1.234 (1.334)\tData 0.300 (0.325)\tLoss 0.4758 (0.2068)\n",
            "\u001b[32m[03/16 19:11:41 BIKE]: \u001b[0mEpoch: [12][40/92], lr: 3.99e-05, eta: 0:35:28\tTime 1.021 (1.317)\tData 0.089 (0.304)\tLoss 0.1553 (0.2031)\n",
            "\u001b[32m[03/16 19:11:53 BIKE]: \u001b[0mEpoch: [12][50/92], lr: 3.96e-05, eta: 0:34:41\tTime 1.233 (1.295)\tData 0.309 (0.288)\tLoss 0.2583 (0.1965)\n",
            "\u001b[32m[03/16 19:12:05 BIKE]: \u001b[0mEpoch: [12][60/92], lr: 3.93e-05, eta: 0:34:03\tTime 1.104 (1.280)\tData 0.156 (0.278)\tLoss 0.1478 (0.1960)\n",
            "\u001b[32m[03/16 19:12:17 BIKE]: \u001b[0mEpoch: [12][70/92], lr: 3.91e-05, eta: 0:33:27\tTime 1.737 (1.265)\tData 0.770 (0.268)\tLoss 0.0515 (0.1876)\n",
            "\u001b[32m[03/16 19:12:28 BIKE]: \u001b[0mEpoch: [12][80/92], lr: 3.88e-05, eta: 0:32:51\tTime 1.748 (1.250)\tData 0.686 (0.261)\tLoss 0.1434 (0.1841)\n",
            "\u001b[32m[03/16 19:12:39 BIKE]: \u001b[0mEpoch: [12][90/92], lr: 3.85e-05, eta: 0:32:14\tTime 0.922 (1.235)\tData 0.036 (0.252)\tLoss 0.2516 (0.1830)\n",
            "\u001b[32m[03/16 19:12:43 BIKE]: \u001b[0mTest: [0/62]\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 19:12:52 BIKE]: \u001b[0mTest: [10/62]\tPrec@1 100.000 (98.864)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 19:13:01 BIKE]: \u001b[0mTest: [20/62]\tPrec@1 96.875 (98.363)\tPrec@5 100.000 (99.702)\n",
            "\u001b[32m[03/16 19:13:10 BIKE]: \u001b[0mTest: [30/62]\tPrec@1 100.000 (98.690)\tPrec@5 100.000 (99.798)\n",
            "\u001b[32m[03/16 19:13:19 BIKE]: \u001b[0mTest: [40/62]\tPrec@1 96.875 (98.476)\tPrec@5 96.875 (99.695)\n",
            "\u001b[32m[03/16 19:13:28 BIKE]: \u001b[0mTest: [50/62]\tPrec@1 87.500 (97.549)\tPrec@5 100.000 (99.755)\n",
            "\u001b[32m[03/16 19:13:36 BIKE]: \u001b[0mTest: [60/62]\tPrec@1 100.000 (97.643)\tPrec@5 100.000 (99.693)\n",
            "\u001b[32m[03/16 19:13:37 BIKE]: \u001b[0mTesting Results: Prec@1 97.678 Prec@5 99.697\n",
            "\u001b[32m[03/16 19:13:37 BIKE]: \u001b[0mTesting: 97.67794043412418/97.778899545684\n",
            "\u001b[32m[03/16 19:13:37 BIKE]: \u001b[0mSaving:\n",
            "\u001b[32m[03/16 19:13:49 BIKE]: \u001b[0mEpoch: [13][0/92], lr: 3.84e-05, eta: 1:17:03\tTime 2.954 (2.954)\tData 1.748 (1.748)\tLoss 0.1886 (0.1886)\n",
            "\u001b[32m[03/16 19:14:02 BIKE]: \u001b[0mEpoch: [13][10/92], lr: 3.81e-05, eta: 0:36:09\tTime 1.364 (1.395)\tData 0.391 (0.377)\tLoss 0.1774 (0.1635)\n",
            "\u001b[32m[03/16 19:14:15 BIKE]: \u001b[0mEpoch: [13][20/92], lr: 3.78e-05, eta: 0:34:47\tTime 1.228 (1.351)\tData 0.144 (0.327)\tLoss 0.2260 (0.1804)\n",
            "\u001b[32m[03/16 19:14:27 BIKE]: \u001b[0mEpoch: [13][30/92], lr: 3.75e-05, eta: 0:33:51\tTime 1.170 (1.323)\tData 0.173 (0.295)\tLoss 0.1119 (0.1890)\n",
            "\u001b[32m[03/16 19:14:40 BIKE]: \u001b[0mEpoch: [13][40/92], lr: 3.73e-05, eta: 0:32:57\tTime 1.182 (1.297)\tData 0.208 (0.272)\tLoss 0.2446 (0.1926)\n",
            "\u001b[32m[03/16 19:14:51 BIKE]: \u001b[0mEpoch: [13][50/92], lr: 3.70e-05, eta: 0:32:12\tTime 1.234 (1.276)\tData 0.297 (0.260)\tLoss 0.1960 (0.1854)\n",
            "\u001b[32m[03/16 19:15:03 BIKE]: \u001b[0mEpoch: [13][60/92], lr: 3.67e-05, eta: 0:31:42\tTime 1.242 (1.264)\tData 0.351 (0.257)\tLoss 0.3017 (0.1864)\n",
            "\u001b[32m[03/16 19:15:15 BIKE]: \u001b[0mEpoch: [13][70/92], lr: 3.63e-05, eta: 0:31:10\tTime 1.655 (1.251)\tData 0.589 (0.250)\tLoss 0.1933 (0.1831)\n",
            "\u001b[32m[03/16 19:15:26 BIKE]: \u001b[0mEpoch: [13][80/92], lr: 3.60e-05, eta: 0:30:35\tTime 1.404 (1.236)\tData 0.357 (0.242)\tLoss 0.1666 (0.1772)\n",
            "\u001b[32m[03/16 19:15:38 BIKE]: \u001b[0mEpoch: [13][90/92], lr: 3.57e-05, eta: 0:30:04\tTime 0.922 (1.223)\tData 0.035 (0.233)\tLoss 0.0817 (0.1723)\n",
            "\u001b[32m[03/16 19:15:41 BIKE]: \u001b[0mTest: [0/62]\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 19:15:51 BIKE]: \u001b[0mTest: [10/62]\tPrec@1 100.000 (98.864)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 19:16:00 BIKE]: \u001b[0mTest: [20/62]\tPrec@1 96.875 (98.363)\tPrec@5 100.000 (99.851)\n",
            "\u001b[32m[03/16 19:16:08 BIKE]: \u001b[0mTest: [30/62]\tPrec@1 100.000 (98.790)\tPrec@5 100.000 (99.899)\n",
            "\u001b[32m[03/16 19:16:17 BIKE]: \u001b[0mTest: [40/62]\tPrec@1 96.875 (98.171)\tPrec@5 100.000 (99.848)\n",
            "\u001b[32m[03/16 19:16:26 BIKE]: \u001b[0mTest: [50/62]\tPrec@1 100.000 (97.978)\tPrec@5 100.000 (99.877)\n",
            "\u001b[32m[03/16 19:16:34 BIKE]: \u001b[0mTest: [60/62]\tPrec@1 100.000 (97.695)\tPrec@5 100.000 (99.744)\n",
            "\u001b[32m[03/16 19:16:35 BIKE]: \u001b[0mTesting Results: Prec@1 97.728 Prec@5 99.748\n",
            "\u001b[32m[03/16 19:16:35 BIKE]: \u001b[0mTesting: 97.72841998990408/97.778899545684\n",
            "\u001b[32m[03/16 19:16:35 BIKE]: \u001b[0mSaving:\n",
            "\u001b[32m[03/16 19:16:46 BIKE]: \u001b[0mEpoch: [14][0/92], lr: 3.56e-05, eta: 1:06:15\tTime 2.699 (2.699)\tData 1.678 (1.678)\tLoss 0.1396 (0.1396)\n",
            "\u001b[32m[03/16 19:16:58 BIKE]: \u001b[0mEpoch: [14][10/92], lr: 3.54e-05, eta: 0:32:23\tTime 1.394 (1.328)\tData 0.367 (0.321)\tLoss 0.0810 (0.1049)\n",
            "\u001b[32m[03/16 19:17:10 BIKE]: \u001b[0mEpoch: [14][20/92], lr: 3.51e-05, eta: 0:31:10\tTime 1.276 (1.287)\tData 0.318 (0.284)\tLoss 0.2387 (0.1151)\n",
            "\u001b[32m[03/16 19:17:23 BIKE]: \u001b[0mEpoch: [14][30/92], lr: 3.47e-05, eta: 0:30:33\tTime 1.128 (1.270)\tData 0.136 (0.254)\tLoss 0.1053 (0.1170)\n",
            "\u001b[32m[03/16 19:17:35 BIKE]: \u001b[0mEpoch: [14][40/92], lr: 3.44e-05, eta: 0:30:12\tTime 1.586 (1.265)\tData 0.663 (0.241)\tLoss 0.1266 (0.1231)\n",
            "\u001b[32m[03/16 19:17:47 BIKE]: \u001b[0mEpoch: [14][50/92], lr: 3.41e-05, eta: 0:29:29\tTime 1.583 (1.244)\tData 0.497 (0.224)\tLoss 0.0299 (0.1155)\n",
            "\u001b[32m[03/16 19:17:57 BIKE]: \u001b[0mEpoch: [14][60/92], lr: 3.38e-05, eta: 0:28:38\tTime 1.126 (1.216)\tData 0.103 (0.207)\tLoss 0.2119 (0.1294)\n",
            "\u001b[32m[03/16 19:18:09 BIKE]: \u001b[0mEpoch: [14][70/92], lr: 3.35e-05, eta: 0:28:17\tTime 1.073 (1.210)\tData 0.121 (0.204)\tLoss 0.3506 (0.1395)\n",
            "\u001b[32m[03/16 19:18:20 BIKE]: \u001b[0mEpoch: [14][80/92], lr: 3.31e-05, eta: 0:27:50\tTime 1.169 (1.200)\tData 0.229 (0.196)\tLoss 0.1056 (0.1352)\n",
            "\u001b[32m[03/16 19:18:32 BIKE]: \u001b[0mEpoch: [14][90/92], lr: 3.28e-05, eta: 0:27:29\tTime 0.926 (1.192)\tData 0.041 (0.190)\tLoss 0.1051 (0.1358)\n",
            "\u001b[32m[03/16 19:18:35 BIKE]: \u001b[0mTest: [0/62]\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 19:18:44 BIKE]: \u001b[0mTest: [10/62]\tPrec@1 100.000 (98.864)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 19:18:53 BIKE]: \u001b[0mTest: [20/62]\tPrec@1 96.875 (98.363)\tPrec@5 100.000 (99.851)\n",
            "\u001b[32m[03/16 19:19:02 BIKE]: \u001b[0mTest: [30/62]\tPrec@1 100.000 (98.589)\tPrec@5 100.000 (99.899)\n",
            "\u001b[32m[03/16 19:19:10 BIKE]: \u001b[0mTest: [40/62]\tPrec@1 96.875 (98.552)\tPrec@5 96.875 (99.771)\n",
            "\u001b[32m[03/16 19:19:19 BIKE]: \u001b[0mTest: [50/62]\tPrec@1 87.500 (98.162)\tPrec@5 100.000 (99.816)\n",
            "\u001b[32m[03/16 19:19:27 BIKE]: \u001b[0mTest: [60/62]\tPrec@1 100.000 (97.951)\tPrec@5 100.000 (99.744)\n",
            "\u001b[32m[03/16 19:19:28 BIKE]: \u001b[0mTesting Results: Prec@1 97.981 Prec@5 99.748\n",
            "\u001b[32m[03/16 19:19:28 BIKE]: \u001b[0mTesting: 97.98081776880363/97.98081776880363\n",
            "\u001b[32m[03/16 19:19:28 BIKE]: \u001b[0mSaving:\n",
            "\u001b[32m[03/16 19:19:48 BIKE]: \u001b[0mEpoch: [15][0/92], lr: 3.27e-05, eta: 1:02:49\tTime 2.730 (2.730)\tData 1.609 (1.609)\tLoss 0.1595 (0.1595)\n",
            "\u001b[32m[03/16 19:20:00 BIKE]: \u001b[0mEpoch: [15][10/92], lr: 3.24e-05, eta: 0:32:13\tTime 1.241 (1.410)\tData 0.237 (0.361)\tLoss 0.0998 (0.1310)\n",
            "\u001b[32m[03/16 19:20:13 BIKE]: \u001b[0mEpoch: [15][20/92], lr: 3.21e-05, eta: 0:30:44\tTime 1.167 (1.355)\tData 0.181 (0.304)\tLoss 0.0684 (0.1266)\n",
            "\u001b[32m[03/16 19:20:26 BIKE]: \u001b[0mEpoch: [15][30/92], lr: 3.18e-05, eta: 0:30:07\tTime 1.292 (1.338)\tData 0.274 (0.284)\tLoss 0.0600 (0.1296)\n",
            "\u001b[32m[03/16 19:20:39 BIKE]: \u001b[0mEpoch: [15][40/92], lr: 3.14e-05, eta: 0:29:41\tTime 1.221 (1.328)\tData 0.208 (0.272)\tLoss 0.4101 (0.1427)\n",
            "\u001b[32m[03/16 19:20:52 BIKE]: \u001b[0mEpoch: [15][50/92], lr: 3.11e-05, eta: 0:29:17\tTime 1.235 (1.321)\tData 0.205 (0.268)\tLoss 0.2112 (0.1442)\n",
            "\u001b[32m[03/16 19:21:05 BIKE]: \u001b[0mEpoch: [15][60/92], lr: 3.08e-05, eta: 0:28:51\tTime 1.243 (1.311)\tData 0.276 (0.258)\tLoss 0.0596 (0.1394)\n",
            "\u001b[32m[03/16 19:21:17 BIKE]: \u001b[0mEpoch: [15][70/92], lr: 3.05e-05, eta: 0:28:15\tTime 0.998 (1.294)\tData 0.094 (0.248)\tLoss 0.1595 (0.1342)\n",
            "\u001b[32m[03/16 19:21:28 BIKE]: \u001b[0mEpoch: [15][80/92], lr: 3.01e-05, eta: 0:27:38\tTime 1.317 (1.275)\tData 0.324 (0.235)\tLoss 0.0739 (0.1327)\n",
            "\u001b[32m[03/16 19:21:39 BIKE]: \u001b[0mEpoch: [15][90/92], lr: 2.98e-05, eta: 0:26:55\tTime 0.961 (1.251)\tData 0.054 (0.219)\tLoss 0.0270 (0.1351)\n",
            "\u001b[32m[03/16 19:21:42 BIKE]: \u001b[0mTest: [0/62]\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 19:21:51 BIKE]: \u001b[0mTest: [10/62]\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 19:22:00 BIKE]: \u001b[0mTest: [20/62]\tPrec@1 96.875 (99.405)\tPrec@5 100.000 (99.851)\n",
            "\u001b[32m[03/16 19:22:08 BIKE]: \u001b[0mTest: [30/62]\tPrec@1 100.000 (99.496)\tPrec@5 100.000 (99.798)\n",
            "\u001b[32m[03/16 19:22:17 BIKE]: \u001b[0mTest: [40/62]\tPrec@1 90.625 (98.933)\tPrec@5 96.875 (99.695)\n",
            "\u001b[32m[03/16 19:22:25 BIKE]: \u001b[0mTest: [50/62]\tPrec@1 100.000 (98.529)\tPrec@5 100.000 (99.755)\n",
            "\u001b[32m[03/16 19:22:34 BIKE]: \u001b[0mTest: [60/62]\tPrec@1 100.000 (98.412)\tPrec@5 100.000 (99.693)\n",
            "\u001b[32m[03/16 19:22:35 BIKE]: \u001b[0mTesting Results: Prec@1 98.435 Prec@5 99.697\n",
            "\u001b[32m[03/16 19:22:35 BIKE]: \u001b[0mTesting: 98.43513377082282/98.43513377082282\n",
            "\u001b[32m[03/16 19:22:35 BIKE]: \u001b[0mSaving:\n",
            "\u001b[32m[03/16 19:23:01 BIKE]: \u001b[0mEpoch: [16][0/92], lr: 2.97e-05, eta: 1:17:26\tTime 3.605 (3.605)\tData 2.458 (2.458)\tLoss 0.1832 (0.1832)\n",
            "\u001b[32m[03/16 19:23:14 BIKE]: \u001b[0mEpoch: [16][10/92], lr: 2.94e-05, eta: 0:32:14\tTime 1.571 (1.513)\tData 0.626 (0.475)\tLoss 0.1159 (0.1426)\n",
            "\u001b[32m[03/16 19:23:27 BIKE]: \u001b[0mEpoch: [16][20/92], lr: 2.90e-05, eta: 0:30:11\tTime 1.884 (1.427)\tData 0.842 (0.380)\tLoss 0.0477 (0.1214)\n",
            "\u001b[32m[03/16 19:23:40 BIKE]: \u001b[0mEpoch: [16][30/92], lr: 2.87e-05, eta: 0:28:49\tTime 1.586 (1.374)\tData 0.507 (0.318)\tLoss 0.0707 (0.1214)\n",
            "\u001b[32m[03/16 19:23:52 BIKE]: \u001b[0mEpoch: [16][40/92], lr: 2.84e-05, eta: 0:28:03\tTime 2.067 (1.348)\tData 0.873 (0.290)\tLoss 0.1418 (0.1217)\n",
            "\u001b[32m[03/16 19:24:05 BIKE]: \u001b[0mEpoch: [16][50/92], lr: 2.80e-05, eta: 0:27:21\tTime 1.695 (1.325)\tData 0.617 (0.276)\tLoss 0.0843 (0.1189)\n",
            "\u001b[32m[03/16 19:24:17 BIKE]: \u001b[0mEpoch: [16][60/92], lr: 2.77e-05, eta: 0:26:48\tTime 1.053 (1.309)\tData 0.078 (0.272)\tLoss 0.0586 (0.1157)\n",
            "\u001b[32m[03/16 19:24:29 BIKE]: \u001b[0mEpoch: [16][70/92], lr: 2.74e-05, eta: 0:26:21\tTime 1.059 (1.297)\tData 0.128 (0.265)\tLoss 0.2726 (0.1193)\n",
            "\u001b[32m[03/16 19:24:41 BIKE]: \u001b[0mEpoch: [16][80/92], lr: 2.70e-05, eta: 0:25:49\tTime 1.108 (1.281)\tData 0.119 (0.256)\tLoss 0.1769 (0.1236)\n",
            "\u001b[32m[03/16 19:24:52 BIKE]: \u001b[0mEpoch: [16][90/92], lr: 2.67e-05, eta: 0:25:14\tTime 0.930 (1.263)\tData 0.035 (0.241)\tLoss 0.1502 (0.1212)\n",
            "\u001b[32m[03/16 19:24:55 BIKE]: \u001b[0mTest: [0/62]\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 19:25:05 BIKE]: \u001b[0mTest: [10/62]\tPrec@1 100.000 (98.864)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 19:25:13 BIKE]: \u001b[0mTest: [20/62]\tPrec@1 96.875 (98.512)\tPrec@5 100.000 (99.851)\n",
            "\u001b[32m[03/16 19:25:22 BIKE]: \u001b[0mTest: [30/62]\tPrec@1 100.000 (98.690)\tPrec@5 100.000 (99.798)\n",
            "\u001b[32m[03/16 19:25:31 BIKE]: \u001b[0mTest: [40/62]\tPrec@1 90.625 (97.942)\tPrec@5 96.875 (99.695)\n",
            "\u001b[32m[03/16 19:25:39 BIKE]: \u001b[0mTest: [50/62]\tPrec@1 100.000 (98.100)\tPrec@5 100.000 (99.755)\n",
            "\u001b[32m[03/16 19:25:48 BIKE]: \u001b[0mTest: [60/62]\tPrec@1 100.000 (97.848)\tPrec@5 100.000 (99.641)\n",
            "\u001b[32m[03/16 19:25:49 BIKE]: \u001b[0mTesting Results: Prec@1 97.880 Prec@5 99.647\n",
            "\u001b[32m[03/16 19:25:49 BIKE]: \u001b[0mTesting: 97.87985865724382/98.43513377082282\n",
            "\u001b[32m[03/16 19:25:49 BIKE]: \u001b[0mSaving:\n",
            "\u001b[32m[03/16 19:26:05 BIKE]: \u001b[0mEpoch: [17][0/92], lr: 2.66e-05, eta: 0:57:02\tTime 2.859 (2.859)\tData 1.778 (1.778)\tLoss 0.0760 (0.0760)\n",
            "\u001b[32m[03/16 19:26:17 BIKE]: \u001b[0mEpoch: [17][10/92], lr: 2.63e-05, eta: 0:27:42\tTime 1.253 (1.401)\tData 0.275 (0.373)\tLoss 0.1688 (0.1065)\n",
            "\u001b[32m[03/16 19:26:30 BIKE]: \u001b[0mEpoch: [17][20/92], lr: 2.59e-05, eta: 0:26:24\tTime 1.124 (1.346)\tData 0.141 (0.312)\tLoss 0.1200 (0.1095)\n",
            "\u001b[32m[03/16 19:26:43 BIKE]: \u001b[0mEpoch: [17][30/92], lr: 2.56e-05, eta: 0:25:45\tTime 1.156 (1.324)\tData 0.127 (0.273)\tLoss 0.1049 (0.1116)\n",
            "\u001b[32m[03/16 19:26:55 BIKE]: \u001b[0mEpoch: [17][40/92], lr: 2.52e-05, eta: 0:24:50\tTime 1.034 (1.288)\tData 0.121 (0.240)\tLoss 0.2123 (0.1110)\n",
            "\u001b[32m[03/16 19:27:06 BIKE]: \u001b[0mEpoch: [17][50/92], lr: 2.49e-05, eta: 0:23:58\tTime 1.063 (1.254)\tData 0.080 (0.214)\tLoss 0.1015 (0.1124)\n",
            "\u001b[32m[03/16 19:27:17 BIKE]: \u001b[0mEpoch: [17][60/92], lr: 2.46e-05, eta: 0:23:25\tTime 1.205 (1.236)\tData 0.162 (0.200)\tLoss 0.2163 (0.1106)\n",
            "\u001b[32m[03/16 19:27:28 BIKE]: \u001b[0mEpoch: [17][70/92], lr: 2.42e-05, eta: 0:22:50\tTime 1.260 (1.216)\tData 0.163 (0.188)\tLoss 0.0772 (0.1091)\n",
            "\u001b[32m[03/16 19:27:39 BIKE]: \u001b[0mEpoch: [17][80/92], lr: 2.39e-05, eta: 0:22:19\tTime 1.022 (1.199)\tData 0.060 (0.178)\tLoss 0.0249 (0.1050)\n",
            "\u001b[32m[03/16 19:27:50 BIKE]: \u001b[0mEpoch: [17][90/92], lr: 2.35e-05, eta: 0:21:59\tTime 0.997 (1.192)\tData 0.102 (0.172)\tLoss 0.2171 (0.1090)\n",
            "\u001b[32m[03/16 19:27:54 BIKE]: \u001b[0mTest: [0/62]\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 19:28:03 BIKE]: \u001b[0mTest: [10/62]\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 19:28:12 BIKE]: \u001b[0mTest: [20/62]\tPrec@1 96.875 (99.405)\tPrec@5 100.000 (99.851)\n",
            "\u001b[32m[03/16 19:28:21 BIKE]: \u001b[0mTest: [30/62]\tPrec@1 100.000 (99.496)\tPrec@5 100.000 (99.798)\n",
            "\u001b[32m[03/16 19:28:30 BIKE]: \u001b[0mTest: [40/62]\tPrec@1 90.625 (98.780)\tPrec@5 96.875 (99.695)\n",
            "\u001b[32m[03/16 19:28:38 BIKE]: \u001b[0mTest: [50/62]\tPrec@1 90.625 (98.162)\tPrec@5 100.000 (99.755)\n",
            "\u001b[32m[03/16 19:28:46 BIKE]: \u001b[0mTest: [60/62]\tPrec@1 100.000 (98.207)\tPrec@5 100.000 (99.641)\n",
            "\u001b[32m[03/16 19:28:47 BIKE]: \u001b[0mTesting Results: Prec@1 98.233 Prec@5 99.647\n",
            "\u001b[32m[03/16 19:28:47 BIKE]: \u001b[0mTesting: 98.23321554770318/98.43513377082282\n",
            "\u001b[32m[03/16 19:28:47 BIKE]: \u001b[0mSaving:\n",
            "\u001b[32m[03/16 19:29:00 BIKE]: \u001b[0mEpoch: [18][0/92], lr: 2.34e-05, eta: 0:49:16\tTime 2.675 (2.675)\tData 1.721 (1.721)\tLoss 0.1418 (0.1418)\n",
            "\u001b[32m[03/16 19:29:12 BIKE]: \u001b[0mEpoch: [18][10/92], lr: 2.31e-05, eta: 0:25:12\tTime 1.094 (1.381)\tData 0.109 (0.395)\tLoss 0.1128 (0.0970)\n",
            "\u001b[32m[03/16 19:29:25 BIKE]: \u001b[0mEpoch: [18][20/92], lr: 2.28e-05, eta: 0:23:43\tTime 1.135 (1.312)\tData 0.168 (0.294)\tLoss 0.0506 (0.1146)\n",
            "\u001b[32m[03/16 19:29:37 BIKE]: \u001b[0mEpoch: [18][30/92], lr: 2.24e-05, eta: 0:22:58\tTime 1.103 (1.283)\tData 0.112 (0.246)\tLoss 0.1036 (0.1228)\n",
            "\u001b[32m[03/16 19:29:49 BIKE]: \u001b[0mEpoch: [18][40/92], lr: 2.21e-05, eta: 0:22:30\tTime 1.032 (1.268)\tData 0.061 (0.227)\tLoss 0.0816 (0.1141)\n",
            "\u001b[32m[03/16 19:30:00 BIKE]: \u001b[0mEpoch: [18][50/92], lr: 2.18e-05, eta: 0:21:44\tTime 1.123 (1.237)\tData 0.128 (0.202)\tLoss 0.1468 (0.1155)\n",
            "\u001b[32m[03/16 19:30:11 BIKE]: \u001b[0mEpoch: [18][60/92], lr: 2.14e-05, eta: 0:21:07\tTime 1.280 (1.213)\tData 0.180 (0.187)\tLoss 0.0724 (0.1142)\n",
            "\u001b[32m[03/16 19:30:22 BIKE]: \u001b[0mEpoch: [18][70/92], lr: 2.11e-05, eta: 0:20:34\tTime 1.026 (1.193)\tData 0.060 (0.175)\tLoss 0.0616 (0.1151)\n",
            "\u001b[32m[03/16 19:30:33 BIKE]: \u001b[0mEpoch: [18][80/92], lr: 2.08e-05, eta: 0:20:14\tTime 1.125 (1.185)\tData 0.116 (0.169)\tLoss 0.0513 (0.1165)\n",
            "\u001b[32m[03/16 19:30:44 BIKE]: \u001b[0mEpoch: [18][90/92], lr: 2.04e-05, eta: 0:19:53\tTime 0.924 (1.176)\tData 0.036 (0.163)\tLoss 0.1099 (0.1133)\n",
            "\u001b[32m[03/16 19:30:47 BIKE]: \u001b[0mTest: [0/62]\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 19:30:57 BIKE]: \u001b[0mTest: [10/62]\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 19:31:05 BIKE]: \u001b[0mTest: [20/62]\tPrec@1 96.875 (99.405)\tPrec@5 100.000 (99.851)\n",
            "\u001b[32m[03/16 19:31:14 BIKE]: \u001b[0mTest: [30/62]\tPrec@1 100.000 (99.496)\tPrec@5 100.000 (99.798)\n",
            "\u001b[32m[03/16 19:31:23 BIKE]: \u001b[0mTest: [40/62]\tPrec@1 96.875 (99.162)\tPrec@5 100.000 (99.848)\n",
            "\u001b[32m[03/16 19:31:31 BIKE]: \u001b[0mTest: [50/62]\tPrec@1 93.750 (98.836)\tPrec@5 100.000 (99.877)\n",
            "\u001b[32m[03/16 19:31:40 BIKE]: \u001b[0mTest: [60/62]\tPrec@1 100.000 (98.719)\tPrec@5 100.000 (99.744)\n",
            "\u001b[32m[03/16 19:31:41 BIKE]: \u001b[0mTesting Results: Prec@1 98.738 Prec@5 99.748\n",
            "\u001b[32m[03/16 19:31:41 BIKE]: \u001b[0mTesting: 98.73801110550227/98.73801110550227\n",
            "\u001b[32m[03/16 19:31:41 BIKE]: \u001b[0mSaving:\n",
            "\u001b[32m[03/16 19:32:03 BIKE]: \u001b[0mEpoch: [19][0/92], lr: 2.03e-05, eta: 0:51:34\tTime 3.055 (3.055)\tData 1.930 (1.930)\tLoss 0.2158 (0.2158)\n",
            "\u001b[32m[03/16 19:32:16 BIKE]: \u001b[0mEpoch: [19][10/92], lr: 2.00e-05, eta: 0:24:33\tTime 1.221 (1.470)\tData 0.241 (0.422)\tLoss 0.1345 (0.1188)\n",
            "\u001b[32m[03/16 19:32:29 BIKE]: \u001b[0mEpoch: [19][20/92], lr: 1.97e-05, eta: 0:22:59\tTime 1.399 (1.389)\tData 0.323 (0.321)\tLoss 0.1388 (0.1163)\n",
            "\u001b[32m[03/16 19:32:42 BIKE]: \u001b[0mEpoch: [19][30/92], lr: 1.93e-05, eta: 0:22:12\tTime 1.434 (1.355)\tData 0.429 (0.272)\tLoss 0.0640 (0.1085)\n",
            "\u001b[32m[03/16 19:32:55 BIKE]: \u001b[0mEpoch: [19][40/92], lr: 1.90e-05, eta: 0:21:44\tTime 1.696 (1.341)\tData 0.547 (0.250)\tLoss 0.1290 (0.1031)\n",
            "\u001b[32m[03/16 19:33:08 BIKE]: \u001b[0mEpoch: [19][50/92], lr: 1.87e-05, eta: 0:21:26\tTime 1.732 (1.336)\tData 0.805 (0.254)\tLoss 0.0703 (0.0979)\n",
            "\u001b[32m[03/16 19:33:20 BIKE]: \u001b[0mEpoch: [19][60/92], lr: 1.84e-05, eta: 0:20:44\tTime 1.505 (1.305)\tData 0.412 (0.238)\tLoss 0.0941 (0.0964)\n",
            "\u001b[32m[03/16 19:33:31 BIKE]: \u001b[0mEpoch: [19][70/92], lr: 1.80e-05, eta: 0:20:01\tTime 1.028 (1.275)\tData 0.080 (0.220)\tLoss 0.1499 (0.0957)\n",
            "\u001b[32m[03/16 19:33:42 BIKE]: \u001b[0mEpoch: [19][80/92], lr: 1.77e-05, eta: 0:19:29\tTime 1.118 (1.254)\tData 0.178 (0.209)\tLoss 0.4109 (0.0981)\n",
            "\u001b[32m[03/16 19:33:53 BIKE]: \u001b[0mEpoch: [19][90/92], lr: 1.74e-05, eta: 0:19:04\tTime 0.925 (1.240)\tData 0.034 (0.203)\tLoss 0.2366 (0.0986)\n",
            "\u001b[32m[03/16 19:33:56 BIKE]: \u001b[0mTest: [0/62]\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 19:34:06 BIKE]: \u001b[0mTest: [10/62]\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 19:34:15 BIKE]: \u001b[0mTest: [20/62]\tPrec@1 96.875 (99.405)\tPrec@5 100.000 (99.851)\n",
            "\u001b[32m[03/16 19:34:23 BIKE]: \u001b[0mTest: [30/62]\tPrec@1 100.000 (99.395)\tPrec@5 100.000 (99.899)\n",
            "\u001b[32m[03/16 19:34:32 BIKE]: \u001b[0mTest: [40/62]\tPrec@1 96.875 (99.314)\tPrec@5 96.875 (99.771)\n",
            "\u001b[32m[03/16 19:34:40 BIKE]: \u001b[0mTest: [50/62]\tPrec@1 96.875 (99.142)\tPrec@5 100.000 (99.816)\n",
            "\u001b[32m[03/16 19:34:49 BIKE]: \u001b[0mTest: [60/62]\tPrec@1 100.000 (99.027)\tPrec@5 100.000 (99.693)\n",
            "\u001b[32m[03/16 19:34:50 BIKE]: \u001b[0mTesting Results: Prec@1 99.041 Prec@5 99.697\n",
            "\u001b[32m[03/16 19:34:50 BIKE]: \u001b[0mTesting: 99.04088844018173/99.04088844018173\n",
            "\u001b[32m[03/16 19:34:50 BIKE]: \u001b[0mSaving:\n",
            "\u001b[32m[03/16 19:35:06 BIKE]: \u001b[0mEpoch: [20][0/92], lr: 1.73e-05, eta: 0:51:02\tTime 3.325 (3.325)\tData 1.980 (1.980)\tLoss 0.0466 (0.0466)\n",
            "\u001b[32m[03/16 19:35:19 BIKE]: \u001b[0mEpoch: [20][10/92], lr: 1.70e-05, eta: 0:22:40\tTime 1.316 (1.493)\tData 0.228 (0.403)\tLoss 0.0917 (0.0883)\n",
            "\u001b[32m[03/16 19:35:32 BIKE]: \u001b[0mEpoch: [20][20/92], lr: 1.67e-05, eta: 0:21:14\tTime 1.272 (1.414)\tData 0.150 (0.348)\tLoss 0.1880 (0.0825)\n",
            "\u001b[32m[03/16 19:35:45 BIKE]: \u001b[0mEpoch: [20][30/92], lr: 1.63e-05, eta: 0:20:24\tTime 1.157 (1.374)\tData 0.130 (0.302)\tLoss 0.0595 (0.0817)\n",
            "\u001b[32m[03/16 19:35:57 BIKE]: \u001b[0mEpoch: [20][40/92], lr: 1.60e-05, eta: 0:19:38\tTime 1.113 (1.338)\tData 0.168 (0.274)\tLoss 0.0935 (0.0751)\n",
            "\u001b[32m[03/16 19:36:11 BIKE]: \u001b[0mEpoch: [20][50/92], lr: 1.57e-05, eta: 0:19:24\tTime 1.163 (1.337)\tData 0.120 (0.282)\tLoss 0.0454 (0.0773)\n",
            "\u001b[32m[03/16 19:36:23 BIKE]: \u001b[0mEpoch: [20][60/92], lr: 1.54e-05, eta: 0:19:02\tTime 1.133 (1.327)\tData 0.196 (0.280)\tLoss 0.1561 (0.0783)\n",
            "\u001b[32m[03/16 19:36:35 BIKE]: \u001b[0mEpoch: [20][70/92], lr: 1.51e-05, eta: 0:18:26\tTime 1.002 (1.300)\tData 0.074 (0.262)\tLoss 0.1318 (0.0821)\n",
            "\u001b[32m[03/16 19:36:46 BIKE]: \u001b[0mEpoch: [20][80/92], lr: 1.48e-05, eta: 0:17:56\tTime 1.227 (1.280)\tData 0.222 (0.245)\tLoss 0.0182 (0.0802)\n",
            "\u001b[32m[03/16 19:36:56 BIKE]: \u001b[0mEpoch: [20][90/92], lr: 1.44e-05, eta: 0:17:21\tTime 0.960 (1.253)\tData 0.057 (0.227)\tLoss 0.0966 (0.0800)\n",
            "\u001b[32m[03/16 19:37:00 BIKE]: \u001b[0mTest: [0/62]\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 19:37:09 BIKE]: \u001b[0mTest: [10/62]\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\u001b[32m[03/16 19:37:17 BIKE]: \u001b[0mTest: [20/62]\tPrec@1 96.875 (99.256)\tPrec@5 100.000 (99.851)\n",
            "\u001b[32m[03/16 19:37:26 BIKE]: \u001b[0mTest: [30/62]\tPrec@1 100.000 (99.395)\tPrec@5 100.000 (99.798)\n",
            "\u001b[32m[03/16 19:37:34 BIKE]: \u001b[0mTest: [40/62]\tPrec@1 96.875 (99.085)\tPrec@5 100.000 (99.848)\n",
            "\u001b[32m[03/16 19:37:43 BIKE]: \u001b[0mTest: [50/62]\tPrec@1 100.000 (98.836)\tPrec@5 100.000 (99.877)\n",
            "\u001b[32m[03/16 19:37:52 BIKE]: \u001b[0mTest: [60/62]\tPrec@1 100.000 (98.822)\tPrec@5 100.000 (99.744)\n",
            "\u001b[32m[03/16 19:37:53 BIKE]: \u001b[0mTesting Results: Prec@1 98.839 Prec@5 99.748\n",
            "\u001b[32m[03/16 19:37:53 BIKE]: \u001b[0mTesting: 98.83897021706208/99.04088844018173\n",
            "\u001b[32m[03/16 19:37:53 BIKE]: \u001b[0mSaving:\n",
            "\u001b[32m[03/16 19:38:02 BIKE]: \u001b[0mEpoch: [21][0/92], lr: 1.44e-05, eta: 0:38:46\tTime 2.806 (2.806)\tData 1.737 (1.737)\tLoss 0.0622 (0.0622)\n",
            "\u001b[32m[03/16 19:38:13 BIKE]: \u001b[0mEpoch: [21][10/92], lr: 1.41e-05, eta: 0:17:58\tTime 1.116 (1.317)\tData 0.106 (0.315)\tLoss 0.2416 (0.0705)\n",
            "\u001b[32m[03/16 19:38:26 BIKE]: \u001b[0mEpoch: [21][20/92], lr: 1.38e-05, eta: 0:17:08\tTime 1.119 (1.271)\tData 0.113 (0.255)\tLoss 0.0956 (0.0804)\n",
            "\u001b[32m[03/16 19:38:38 BIKE]: \u001b[0mEpoch: [21][30/92], lr: 1.35e-05, eta: 0:16:42\tTime 1.028 (1.254)\tData 0.058 (0.230)\tLoss 0.0432 (0.0669)\n",
            "\u001b[32m[03/16 19:38:50 BIKE]: \u001b[0mEpoch: [21][40/92], lr: 1.32e-05, eta: 0:16:18\tTime 0.991 (1.240)\tData 0.058 (0.209)\tLoss 0.0357 (0.0720)\n",
            "\u001b[32m[03/16 19:39:01 BIKE]: \u001b[0mEpoch: [21][50/92], lr: 1.29e-05, eta: 0:15:53\tTime 1.003 (1.224)\tData 0.058 (0.197)\tLoss 0.1862 (0.0749)\n",
            "\u001b[32m[03/16 19:39:12 BIKE]: \u001b[0mEpoch: [21][60/92], lr: 1.26e-05, eta: 0:15:24\tTime 1.361 (1.202)\tData 0.275 (0.182)\tLoss 0.3724 (0.0846)\n",
            "\u001b[32m[03/16 19:39:23 BIKE]: \u001b[0mEpoch: [21][70/92], lr: 1.23e-05, eta: 0:15:01\tTime 1.113 (1.188)\tData 0.113 (0.175)\tLoss 0.2535 (0.0890)\n",
            "\u001b[32m[03/16 19:39:35 BIKE]: \u001b[0mEpoch: [21][80/92], lr: 1.20e-05, eta: 0:14:43\tTime 1.011 (1.180)\tData 0.066 (0.169)\tLoss 0.0788 (0.0872)\n",
            "\u001b[32m[03/16 19:39:46 BIKE]: \u001b[0mEpoch: [21][90/92], lr: 1.17e-05, eta: 0:14:26\tTime 0.925 (1.172)\tData 0.036 (0.163)\tLoss 0.0698 (0.0854)\n",
            "\u001b[32m[03/16 19:39:49 BIKE]: \u001b[0mTest: [0/62]\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/BIKE\n",
        "\n",
        "config = 'configs/k400/k400_train_rgb_vitb-32-f8.yaml'\n",
        "weight = '/content/drive/MyDrive/BIKE/exps/k400/ViT-B/32/20240314_111055/model_best.pt'\n",
        "!python -m torch.distributed.launch --nproc_per_node=1 \\\n",
        "    test.py --config ${config} --weights ${weight}"
      ],
      "metadata": {
        "id": "ZdsW_Ihv7qHW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f6fc3b4-e1ff-4385-a4f1-e74e2684c7fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/BIKE\n",
            "/usr/local/lib/python3.10/dist-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated\n",
            "and will be removed in future. Use torchrun.\n",
            "Note that --use_env is set by default in torchrun.\n",
            "If your script expects `--local_rank` argument to be set, please\n",
            "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
            "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
            "further instructions\n",
            "\n",
            "  warnings.warn(\n",
            "dropout used:[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "dropout used:[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "loading clip pretrained model!\n",
            "layer=6\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
            "  warnings.warn(\n",
            "video number:1981\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "Test: [0/62], average 0.2883 sec/video \tPrec@1 0.000 (0.000)\tPrec@5 12.500 (12.500)\n",
            "Test: [10/62], average 0.0453 sec/video \tPrec@1 3.125 (2.841)\tPrec@5 31.250 (26.136)\n",
            "Test: [20/62], average 0.0376 sec/video \tPrec@1 0.000 (2.381)\tPrec@5 12.500 (24.702)\n",
            "Test: [30/62], average 0.0340 sec/video \tPrec@1 0.000 (3.024)\tPrec@5 18.750 (24.899)\n",
            "Test: [40/62], average 0.0326 sec/video \tPrec@1 3.125 (2.668)\tPrec@5 21.875 (24.695)\n",
            "Test: [50/62], average 0.0322 sec/video \tPrec@1 3.125 (2.941)\tPrec@5 31.250 (25.735)\n",
            "Test: [60/62], average 0.0313 sec/video \tPrec@1 3.125 (3.176)\tPrec@5 15.625 (25.973)\n",
            "-----Evaluation is finished------\n",
            "Overall Prec@1 3.231% Prec@5 26.199%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BZu69D51f2Yi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}